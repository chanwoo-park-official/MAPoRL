[2024-07-10 04:02:33,417] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0710 04:02:34.322000 140567794980672 torch/distributed/run.py:757] 
W0710 04:02:34.322000 140567794980672 torch/distributed/run.py:757] *****************************************
W0710 04:02:34.322000 140567794980672 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0710 04:02:34.322000 140567794980672 torch/distributed/run.py:757] *****************************************
[2024-07-10 04:02:39,271] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-07-10 04:02:39,389] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-10 04:02:39,541] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 04:02:39,585] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 04:02:39,592] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 04:02:39,592] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 04:02:39,597] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 04:02:39,598] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-10 04:02:39,825] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:39,877] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,054] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,054] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-10 04:02:40,081] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,083] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,088] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,114] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 04:02:40,116] [INFO] [comm.py:637:init_distributed] cdb=None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
{'input_ids': [9242, 7941, 1703, 37, 1433, 27, 391, 16, 22842, 16458, 187, 187, 53, 43561, 27, 309, 313, 71, 16, 1423, 10, 452, 281, 4677, 562, 604, 309, 971, 281, 1335, 871, 841, 6838, 390, 417, 285, 651, 9239, 281, 3590, 50124, 187, 187, 15743, 27, 3105, 2119, 604, 436, 14125, 1060, 533, 352, 434, 4409, 247, 1611, 15, 2490, 187, 8116, 19192, 27, 187, 3039, 309, 313, 71, 16, 1423, 10, 2427, 949, 619, 806, 1524, 2740, 484, 374, 1107, 3622, 984, 344, 3058, 2317, 846, 247, 807, 273, 13597, 687, 395, 50276, 262, 37099, 479, 625, 685, 309, 1869, 15, 733, 369, 247, 19201, 673, 275, 619, 1495, 1955, 281, 3811, 342, 619, 3101, 285, 4720, 1907, 253, 4839, 281, 2624, 617, 562, 273, 619, 1495, 15, 309, 476, 11476, 984, 273, 352, 369, 271, 8991, 24028, 285, 436, 5599, 369, 6474, 285, 1904, 626, 871, 849, 281, 2968, 342, 479, 15, 844, 7402, 407, 779, 17816, 323, 247, 1770, 390, 594, 846, 1469, 281, 247, 16365, 342, 619, 3858, 15, 2091, 309, 1158, 896, 309, 5730, 344, 816, 7402, 15, 1893, 846, 344, 7402, 352, 2879, 619, 9454, 309, 9606, 533, 619, 3858, 6518, 479, 949, 352, 285, 309, 1694, 8314, 273, 3253, 432, 779, 2112, 342, 9968, 3057, 15, 2490, 187, 4125, 27, 7850, 644, 2761, 495, 1107, 1024, 285, 309, 1849, 12759, 1805, 846, 2258, 23708, 285, 11134, 3270, 1305, 560, 1103, 15, 2752, 3101, 556, 644, 562, 273, 619, 1495, 1580, 840, 594, 627, 434, 644, 47899, 273, 4780, 15, 16688, 10046, 846, 4715, 690, 15880, 627, 644, 625, 12288, 670, 326, 673, 273, 619, 1495, 533, 672, 309, 923, 779, 390, 247, 5406, 3253, 3249, 896, 15, 380, 14021, 285, 12959, 3324, 479, 896, 1066, 15, 2490, 187, 8389, 3858, 313, 15617, 6838, 10, 403, 327, 619, 36979, 984, 359, 755, 2112, 973, 534, 310, 1892, 281, 1089, 285, 309, 871, 597, 1833, 1900, 452, 521, 896, 15, 1292, 6523, 779, 275, 247, 5406, 390, 5015, 281, 779, 387, 247, 5008, 1907, 247, 7827, 310, 10458, 15, 330, 11729, 11449, 273, 619, 1655, 22273, 310, 1633, 309, 971, 281, 3693, 15, 2490, 187, 2598, 309, 1849, 644, 4680, 326, 309, 452, 281, 2624, 3057, 342, 841, 6838, 984, 352, 434, 673, 281, 2118, 327, 984, 352, 434, 33696, 15, 733, 434, 1682, 281, 3693, 779, 347, 973, 15, 1292, 588, 597, 320, 20423, 264, 32, 7395, 597, 2997, 352, 32, 1680, 627, 1469, 281, 320, 19328, 1255, 32, 309, 1353, 417, 2119, 604, 352, 434, 253, 987, 281, 513, 285, 812, 897, 690, 3345, 11626, 15, 187, 187, 14135, 28, 4976, 27], 'lengths': 438}
[2024-07-10 04:02:58,692] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-07-10 04:03:11,523] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-10 04:03:11,525] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-10 04:03:11,525] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-10 04:03:11,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-10 04:03:11,545] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-10 04:03:11,545] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-10 04:03:11,545] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-07-10 04:03:11,545] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-07-10 04:03:11,545] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-07-10 04:03:11,545] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-07-10 04:03:19,949] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states
[2024-07-10 04:03:19,950] [INFO] [utils.py:780:see_memory_usage] MA 4.61 GB         Max_MA 4.61 GB         CA 4.88 GB         Max_CA 5 GB 
[2024-07-10 04:03:19,950] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 88.47 GB, percent = 7.9%
[2024-07-10 04:03:20,132] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states
[2024-07-10 04:03:20,132] [INFO] [utils.py:780:see_memory_usage] MA 4.61 GB         Max_MA 5.51 GB         CA 5.78 GB         Max_CA 6 GB 
[2024-07-10 04:03:20,183] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 86.24 GB, percent = 7.7%
[2024-07-10 04:03:20,184] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-07-10 04:03:20,362] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer
[2024-07-10 04:03:20,363] [INFO] [utils.py:780:see_memory_usage] MA 4.61 GB         Max_MA 4.61 GB         CA 5.78 GB         Max_CA 6 GB 
[2024-07-10 04:03:20,363] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 81.45 GB, percent = 7.3%
[2024-07-10 04:03:20,365] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-07-10 04:03:20,365] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-10 04:03:20,365] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-10 04:03:20,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-06, 3e-06], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-07-10 04:03:20,366] [INFO] [config.py:995:print] DeepSpeedEngine configuration:
[2024-07-10 04:03:20,366] [INFO] [config.py:999:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   amp_enabled .................. False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   amp_params ................... False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   bfloat16_enabled ............. True
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   bfloat16_immediate_grad_update  False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   checkpoint_parallel_write_pipeline  False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   checkpoint_tag_validation_enabled  True
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   checkpoint_tag_validation_fail  False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd81c11cbe0>
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   communication_data_type ...... None
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   curriculum_enabled_legacy .... False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   curriculum_params_legacy ..... False
[2024-07-10 04:03:20,367] [INFO] [config.py:999:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   data_efficiency_enabled ...... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   dataloader_drop_last ......... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   disable_allgather ............ False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   dump_state ................... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   dynamic_loss_scale_args ...... None
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_enabled ........... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_layer_num ......... 0
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_max_iter .......... 100
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_stability ......... 1e-06
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_tol ............... 0.01
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   eigenvalue_verbose ........... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   elasticity_enabled ........... False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   fp16_auto_cast ............... None
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   fp16_enabled ................. False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   fp16_master_weights_and_gradients  False
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   global_rank .................. 0
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   grad_accum_dtype ............. None
[2024-07-10 04:03:20,368] [INFO] [config.py:999:print]   gradient_accumulation_steps .. 4
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   gradient_clipping ............ 1.0
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   gradient_predivide_factor .... 1.0
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   graph_harvesting ............. False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   initial_dynamic_scale ........ 1
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   load_universal_checkpoint .... False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   loss_scale ................... 1.0
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   memory_breakdown ............. False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   mics_hierarchial_params_gather  False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   mics_shard_size .............. -1
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   optimizer_legacy_fusion ...... False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   optimizer_name ............... None
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   optimizer_params ............. None
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   pld_enabled .................. False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   pld_params ................... False
[2024-07-10 04:03:20,369] [INFO] [config.py:999:print]   prescale_gradients ........... False
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   scheduler_name ............... None
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   scheduler_params ............. None
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   sparse_attention ............. None
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   sparse_gradients_enabled ..... False
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   steps_per_print .............. inf
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   train_batch_size ............. 2048
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   train_micro_batch_size_per_gpu  64
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   use_data_before_expert_parallel_  False
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   use_node_local_storage ....... False
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   wall_clock_breakdown ......... False
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   weight_quantization_config ... None
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   world_size ................... 8
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   zero_allow_untested_optimizer  True
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   zero_enabled ................. True
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-10 04:03:20,370] [INFO] [config.py:999:print]   zero_optimization_stage ...... 2
[2024-07-10 04:03:20,371] [INFO] [config.py:985:print_user_config]   json = {
    "train_batch_size": 2.048000e+03, 
    "train_micro_batch_size_per_gpu": 64, 
    "gradient_accumulation_steps": 4, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[2024-07-10 04:03:20,371] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-07-10 04:03:26,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-10 04:03:26,037] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-07-10 04:03:26,291] [INFO] [utils.py:779:see_memory_usage] begin bf16_optimizer
[2024-07-10 04:03:26,292] [INFO] [utils.py:780:see_memory_usage] MA 6.38 GB         Max_MA 6.38 GB         CA 6.47 GB         Max_CA 6 GB 
[2024-07-10 04:03:26,293] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 45.76 GB, percent = 4.1%
[2024-07-10 04:03:26,466] [INFO] [utils.py:779:see_memory_usage] end bf16_optimizer
[2024-07-10 04:03:26,467] [INFO] [utils.py:780:see_memory_usage] MA 6.38 GB         Max_MA 6.38 GB         CA 6.47 GB         Max_CA 6 GB 
[2024-07-10 04:03:26,467] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 44.45 GB, percent = 4.0%
[2024-07-10 04:03:26,467] [INFO] [config.py:995:print] DeepSpeedEngine configuration:
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   amp_enabled .................. False
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   amp_params ................... False
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   bfloat16_enabled ............. True
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   bfloat16_immediate_grad_update  False
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   checkpoint_parallel_write_pipeline  False
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   checkpoint_tag_validation_enabled  True
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   checkpoint_tag_validation_fail  False
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd80fa5a950>
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   communication_data_type ...... None
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-10 04:03:26,468] [INFO] [config.py:999:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   curriculum_enabled_legacy .... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   curriculum_params_legacy ..... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   data_efficiency_enabled ...... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   dataloader_drop_last ......... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   disable_allgather ............ False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   dump_state ................... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   dynamic_loss_scale_args ...... None
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_enabled ........... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_layer_num ......... 0
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_max_iter .......... 100
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_stability ......... 1e-06
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_tol ............... 0.01
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   eigenvalue_verbose ........... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   elasticity_enabled ........... False
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   fp16_auto_cast ............... None
[2024-07-10 04:03:26,469] [INFO] [config.py:999:print]   fp16_enabled ................. False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   fp16_master_weights_and_gradients  False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   global_rank .................. 0
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   grad_accum_dtype ............. None
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   gradient_accumulation_steps .. 1
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   gradient_clipping ............ 0.0
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   gradient_predivide_factor .... 1.0
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   graph_harvesting ............. False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   initial_dynamic_scale ........ 1
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   load_universal_checkpoint .... False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   loss_scale ................... 1.0
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   memory_breakdown ............. False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   mics_hierarchial_params_gather  False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   mics_shard_size .............. -1
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   optimizer_legacy_fusion ...... False
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   optimizer_name ............... None
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   optimizer_params ............. None
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-10 04:03:26,470] [INFO] [config.py:999:print]   pld_enabled .................. False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   pld_params ................... False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   prescale_gradients ........... False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   scheduler_name ............... None
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   scheduler_params ............. None
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   sparse_attention ............. None
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   sparse_gradients_enabled ..... False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   steps_per_print .............. 10
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   train_batch_size ............. 128
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   train_micro_batch_size_per_gpu  16
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   use_data_before_expert_parallel_  False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   use_node_local_storage ....... False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   wall_clock_breakdown ......... False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   weight_quantization_config ... None
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   world_size ................... 8
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   zero_allow_untested_optimizer  False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   zero_enabled ................. False
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-10 04:03:26,471] [INFO] [config.py:999:print]   zero_optimization_stage ...... 0
[2024-07-10 04:03:26,472] [INFO] [config.py:985:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 16, 
    "bf16": {
        "enabled": true
    }, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false
}
[2024-07-10 04:03:26,472] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-07-10 04:03:32,166] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-10 04:03:32,167] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-07-10 04:03:32,393] [INFO] [utils.py:779:see_memory_usage] begin bf16_optimizer
[2024-07-10 04:03:32,394] [INFO] [utils.py:780:see_memory_usage] MA 8.33 GB         Max_MA 8.33 GB         CA 8.36 GB         Max_CA 8 GB 
[2024-07-10 04:03:32,394] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 34.5 GB, percent = 3.1%
[2024-07-10 04:03:32,525] [INFO] [utils.py:779:see_memory_usage] end bf16_optimizer
[2024-07-10 04:03:32,526] [INFO] [utils.py:780:see_memory_usage] MA 8.33 GB         Max_MA 8.33 GB         CA 8.36 GB         Max_CA 8 GB 
[2024-07-10 04:03:32,526] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 34.5 GB, percent = 3.1%
[2024-07-10 04:03:32,526] [INFO] [config.py:995:print] DeepSpeedEngine configuration:
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   amp_enabled .................. False
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   amp_params ................... False
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   bfloat16_enabled ............. True
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   bfloat16_immediate_grad_update  False
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   checkpoint_parallel_write_pipeline  False
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   checkpoint_tag_validation_enabled  True
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   checkpoint_tag_validation_fail  False
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd80fa17df0>
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   communication_data_type ...... None
[2024-07-10 04:03:32,527] [INFO] [config.py:999:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   curriculum_enabled_legacy .... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   curriculum_params_legacy ..... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   data_efficiency_enabled ...... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   dataloader_drop_last ......... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   disable_allgather ............ False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   dump_state ................... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   dynamic_loss_scale_args ...... None
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_enabled ........... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_layer_num ......... 0
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_max_iter .......... 100
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_stability ......... 1e-06
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_tol ............... 0.01
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   eigenvalue_verbose ........... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   elasticity_enabled ........... False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   fp16_auto_cast ............... None
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   fp16_enabled ................. False
[2024-07-10 04:03:32,528] [INFO] [config.py:999:print]   fp16_master_weights_and_gradients  False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   global_rank .................. 0
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   grad_accum_dtype ............. None
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   gradient_accumulation_steps .. 1
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   gradient_clipping ............ 0.0
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   gradient_predivide_factor .... 1.0
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   graph_harvesting ............. False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   initial_dynamic_scale ........ 1
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   load_universal_checkpoint .... False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   loss_scale ................... 1.0
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   memory_breakdown ............. False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   mics_hierarchial_params_gather  False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   mics_shard_size .............. -1
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   optimizer_legacy_fusion ...... False
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   optimizer_name ............... None
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   optimizer_params ............. None
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-10 04:03:32,529] [INFO] [config.py:999:print]   pld_enabled .................. False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   pld_params ................... False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   prescale_gradients ........... False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   scheduler_name ............... None
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   scheduler_params ............. None
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   sparse_attention ............. None
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   sparse_gradients_enabled ..... False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   steps_per_print .............. 10
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   train_batch_size ............. 128
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   train_micro_batch_size_per_gpu  16
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   use_data_before_expert_parallel_  False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   use_node_local_storage ....... False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   wall_clock_breakdown ......... False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   weight_quantization_config ... None
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   world_size ................... 8
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   zero_allow_untested_optimizer  False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   zero_enabled ................. False
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-10 04:03:32,530] [INFO] [config.py:999:print]   zero_optimization_stage ...... 0
[2024-07-10 04:03:32,530] [INFO] [config.py:985:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 16, 
    "bf16": {
        "enabled": true
    }, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false
}
===training policy===
torch.Size([16, 500]) query shape line 036
500torch.Size([16, 510])  query line 308query shape line 036

500 query line 308
500 query line 308510
 query line 308
500 510query line 308 
query line 308
500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

500510  query line 308query line 308

510 query line 308
510 query line 308
torch.Size([16, 488]) query shape line 036
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
torch.Size([16, 507]) query shape line 036
507 query line 308
507 query line 308
torch.Size([16, 508])507  query shape line 036query line 308

507 query line 308
507 query line 308
507 query line 308
507508  query line 308query line 308

507 508query line 308 
query line 308
507 508query line 308 
query line 308
507 508query line 308 
query line 308
507 508query line 308 
query line 308
507 508query line 308 
query line 308
507 508query line 308 
query line 308
507 508query line 308 
query line 308
507508  query line 308query line 308

507 508query line 308 
query line 308
508 query line 308
508 torch.Size([16, 512])query line 308 
query shape line 036508
 query line 308
508 query line 308
508 query line 308
508 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
torch.Size([16, 506]) query shape line 036
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
tensor([[    0,     0,     0,  ...,    13,   533,   672],
        [    0,     0,     0,  ...,    13,   326,   309],
        [    0,     0,     0,  ...,   776, 10450,   323],
        ...,
        [    0,     0,     0,  ...,   644,  4352,   327],
        [    0,     0,     0,  ...,   604,   309,   717],
        [    0,     0,     0,  ...,   452,   247,  1663]], device='cuda:2') input_ids
tensor([[    0,     0,     0,  ..., 33649,   310,  1633],
        [ 9242,  7941,  1703,  ..., 16525, 16525, 16525],
        [    0,     0,     0,  ...,   187,  5498,   318],
        ...,
        [    0,     0,     0,  ...,   247,  2257,   273],
        [    0,  9242,  7941,  ...,   619, 19609,    15],
        [    0,     0,     0,  ...,   616, 42500,    13]], device='cuda:5') input_ids
tensor([[[-0.1436],
         [-0.1436],
         [-0.1436],
         ...,
         [ 1.4688],
         [ 1.5703],
         [ 2.0938]],

        [[ 1.2109],
         [ 1.2109],
         [ 1.2109],
         ...,
         [ 0.4219],
         [-0.0248],
         [-2.3281]],

        [[-0.4316],
         [-0.4316],
         [-0.4316],
         ...,
         [-1.1016],
         [ 0.6406],
         [ 0.3750]],

        ...,

        [[-1.5625],
         [-1.5625],
         [-1.5625],
         ...,
         [-4.2188],
         [-2.9688],
         [-2.7812]],

        [[ 0.2148],
         [ 0.2148],
         [ 0.2148],
         ...,
         [-1.8203],
         [-2.8750],
         [-2.7656]],

        [[ 1.9688],
         [ 1.9688],
         [ 1.9688],
         ...,
         [-0.0159],
         [-0.0664],
         [ 0.1543]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([ 2.0938, -2.3281,  0.3750,  0.6914,  0.6680, -0.4805,  1.3984,  0.3730,
         4.0312, -4.2812,  1.8828, -1.2031, -0.4590, -2.7812, -2.7656,  0.1543],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562,
        562, 562], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:2') input_ids
tensor([[   0,    0,    0,  ...,  275,  253, 4310],
        [9242, 7941, 1703,  ..., 3801,   54, 2896],
        [   0,    0,    0,  ..., 7408,   15, 1500],
        ...,
        [   0,    0,    0,  ...,  627,  667, 1039],
        [   0,    0,    0,  ...,  667,  581, 1469],
        [   0,    0,    0,  ...,  670,  619,  270]], device='cuda:7') input_ids
tensor([[   0,    0,    0,  ...,  644, 2820,  281],
        [   0,    0,    0,  ...,  984,  352, 1833],
        [   0,    0,    0,  ...,  871,  604,  309],
        ...,
        [   0,    0,    0,  ..., 4857,  432,  619],
        [9242, 7941, 1703,  ...,  689,  619, 7231],
        [   0,    0,    0,  ...,  597,  497,  987]], device='cuda:1') input_ids
tensor([[    0,     0,     0,  ...,   755,   779,   281],
        [    0,     0,     0,  ...,    14, 22179,   763],
        [    0,     0,     0,  ...,  4489, 30961,   285],
        ...,
        [    0,     0,     0,  ...,  1335, 12088,   281],
        [    0,     0,     0,  ...,   309,  1353,   417],
        [    0,     0,     0,  ...,  5015,   281,   617]], device='cuda:0') input_ids
tensor([[    0,     0,     0,  ...,   309,  3153,   342],
        [    0,     0,     0,  ..., 50276,  2347,   476],
        [    0,     0,     0,  ...,   310,   689, 29519],
        ...,
        [    0,     0,     0,  ...,   807,  1711,    15],
        [    0,     0,     0,  ...,    15,   187,   187],
        [    0,     0,     0,  ...,  1353,   417,  2119]], device='cuda:6') input_ids
tensor([[   0,    0,    0,  ...,  253,  516, 2056],
        [   0,    0,    0,  ...,   27,  187,  187],
        [   0,    0,    0,  ...,   32,    0,    0],
        ...,
        [   0,    0,    0,  ...,  285,  309, 1353],
        [   0,    0,    0,  ...,  323, 3289, 3767],
        [   0,    0,    0,  ..., 1943, 2523,  285]], device='cuda:3') input_ids
tensor([[[-1.4453],
         [-1.4453],
         [-1.4453],
         ...,
         [ 3.3750],
         [ 3.3750],
         [ 3.3750]],

        [[ 0.9453],
         [ 0.9453],
         [ 0.9453],
         ...,
         [ 0.5234],
         [ 0.5234],
         [ 0.5234]],

        [[-1.0000],
         [-1.0000],
         [-1.0000],
         ...,
         [ 1.5156],
         [ 1.5156],
         [ 1.5156]],

        ...,

        [[-1.2500],
         [-1.2500],
         [-1.2500],
         ...,
         [-0.0148],
         [-0.0148],
         [-0.0148]],

        [[ 1.2969],
         [ 1.2969],
         [ 1.2969],
         ...,
         [ 3.5156],
         [ 3.5156],
         [ 3.5156]],

        [[ 0.7578],
         [ 0.7578],
         [ 0.7578],
         ...,
         [ 2.4219],
         [ 2.4219],
         [ 2.4219]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([3.1094, 0.4531, 1.2734, 0.9453, 4.9062, 4.3438, 4.5000, 2.7344, 2.1719,
        1.6484, 1.8828, 2.4531, 0.9492, 0.0220, 3.5156, 2.0938],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([545, 525, 540, 544, 546, 534, 535, 554, 530, 539, 562, 540, 551, 536,
        540, 541], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
torch.Size([16, 510]) query shape line 036
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
tensor([[[-0.4473],
         [-0.4473],
         [-0.4473],
         ...,
         [-1.9141],
         [-2.8125],
         [-0.3594]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 0.1475],
         [ 0.0605],
         [ 0.1235]],

        [[-0.1729],
         [-0.1729],
         [-0.1729],
         ...,
         [ 2.6094],
         [ 0.1904],
         [-1.8516]],

        ...,

        [[-0.3652],
         [-0.3652],
         [-0.3652],
         ...,
         [-2.4688],
         [-0.2441],
         [ 0.0233]],

        [[ 0.7969],
         [ 0.2969],
         [-1.2031],
         ...,
         [ 0.9180],
         [ 2.7188],
         [ 4.4375]],

        [[-1.4922],
         [-1.4922],
         [-1.4922],
         ...,
         [-0.3633],
         [ 1.2031],
         [ 0.0150]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-0.3594,  0.1235, -1.8516,  0.9570,  2.7500, -3.8281,  0.4023,  1.5312,
         4.1562,  0.7109,  0.4453,  0.0332,  1.0938,  0.0233,  4.4375,  0.0150],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [9242, 7941, 1703,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        ...,
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0, 9242, 7941,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:5') input_ids
tensor([[[-0.5742],
         [-0.5742],
         [-0.5742],
         ...,
         [-2.3438],
         [-0.7656],
         [-5.3125]],

        [[ 1.7969],
         [ 1.7969],
         [ 1.7969],
         ...,
         [ 1.1094],
         [ 0.2402],
         [ 0.2402]],

        [[ 0.2910],
         [ 0.2910],
         [ 0.2910],
         ...,
         [-2.5469],
         [-0.6914],
         [-2.8125]],

        ...,

        [[ 1.3516],
         [ 1.3516],
         [ 1.3516],
         ...,
         [ 0.2617],
         [ 0.0654],
         [-0.5703]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [-0.6797],
         [-1.7656],
         [ 0.6133]],

        [[-0.1445],
         [-0.1445],
         [-0.1445],
         ...,
         [-3.6562],
         [-2.6875],
         [-1.1953]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([-5.3125,  0.2402, -2.8125, -0.7969, -3.2656,  0.0972,  2.5469, -2.2188,
        -4.1875,  1.5312,  5.9688, -3.9688,  1.2266, -0.5703,  0.6133, -1.1953],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559,
        559, 559], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
tensor([[[ 0.8672],
         [ 0.8672],
         [ 0.8672],
         ...,
         [ 0.4688],
         [ 0.5273],
         [ 2.6406]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 0.1152],
         [-0.7773],
         [-2.4219]],

        [[-0.5156],
         [-0.5156],
         [-0.5156],
         ...,
         [ 1.4922],
         [ 1.3516],
         [-2.1562]],

        ...,

        [[ 1.4609],
         [ 1.4609],
         [ 1.4609],
         ...,
         [ 0.3867],
         [-0.3223],
         [-0.2559]],

        [[ 2.4062],
         [ 2.4062],
         [ 2.4062],
         ...,
         [ 0.8633],
         [-2.7969],
         [-0.2246]],

        [[ 0.9688],
         [ 0.9688],
         [ 0.9688],
         ...,
         [-2.1094],
         [-2.7188],
         [ 0.8320]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        ...,
        [   0,    0,    0,  ...,    0,    0,    0],
        [9242, 7941, 1703,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:1') input_ids
tensor([ 2.6406, -2.4219, -2.1562,  0.0236, -1.5156,  1.3438,  4.5000,  1.3125,
        -2.7812, -1.2812, -4.9062,  0.9258,  2.8594, -0.2559, -0.2246,  0.8320],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564,
        564, 564], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [9242, 7941, 1703,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        ...,
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:7') input_ids
tensor([[[ 4.7852e-01],
         [ 4.7852e-01],
         [ 4.7852e-01],
         ...,
         [-1.2500e+00],
         [-1.1719e+00],
         [-4.0000e+00]],

        [[ 1.3281e-01],
         [ 1.3281e-01],
         [ 1.3281e-01],
         ...,
         [-5.3906e-01],
         [ 1.2656e+00],
         [ 1.1250e+00]],

        [[-1.8984e+00],
         [-1.8984e+00],
         [-1.8984e+00],
         ...,
         [-2.5000e+00],
         [ 3.1836e-01],
         [-2.6875e+00]],

        ...,

        [[ 5.8594e-01],
         [ 5.8594e-01],
         [ 5.8594e-01],
         ...,
         [-1.2812e+00],
         [-8.9844e-01],
         [ 9.6484e-01]],

        [[ 1.8906e+00],
         [ 1.8906e+00],
         [ 1.8906e+00],
         ...,
         [ 2.6094e+00],
         [ 9.4922e-01],
         [ 1.0938e+00]],

        [[ 1.6719e+00],
         [ 1.6719e+00],
         [ 1.6719e+00],
         ...,
         [-2.1973e-03],
         [ 1.5430e-01],
         [-2.9219e+00]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([[[-1.0547],
         [-1.0547],
         [-1.0547],
         ...,
         [-3.0156],
         [-1.6953],
         [-1.7891]],

        [[ 0.4551],
         [ 0.4551],
         [ 0.4551],
         ...,
         [ 3.9531],
         [ 1.7734],
         [ 1.6406]],

        [[-0.8125],
         [-0.8125],
         [-0.8125],
         ...,
         [-1.8125],
         [-0.3086],
         [-0.5469]],

        ...,

        [[-0.3418],
         [-0.3418],
         [-0.3418],
         ...,
         [-0.3223],
         [ 2.8906],
         [ 1.0703]],

        [[ 0.5039],
         [ 0.5039],
         [ 0.5039],
         ...,
         [-0.8945],
         [ 1.5938],
         [ 2.4531]],

        [[-0.6094],
         [-0.6094],
         [-0.6094],
         ...,
         [-3.7344],
         [-4.5000],
         [-4.6250]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([[    0,     0,     0,  ...,   417,  7477,    15],
        [    0,     0,     0,  ...,  4886,   281,   253],
        [    0,     0,     0,  ...,  1390,  2954,    13],
        ...,
        [    0,     0,     0,  ...,  2181,    13,   533],
        [    0,     0,     0,  ..., 23313,   661,   275],
        [    0,     0,     0,  ...,   281,   479,    15]], device='cuda:4') input_ids
tensor([-1.7891,  1.6406, -0.5469,  1.0156, -1.0391, -0.3535, -1.1406, -0.8008,
        -0.5430,  0.3809, -3.4844,  1.8672,  0.8203,  1.0703,  2.4531, -4.6250],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([-4.0000,  1.1250, -2.6875,  5.5312, -2.8438, -0.6836, -0.6875,  0.0962,
        -1.4844, -6.9375,  0.3457, -0.9727, -3.2031,  0.9648,  1.0938, -2.9219],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540,
        540, 540], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
tensor([552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552,
        552, 552], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:6') input_ids
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') input_ids
tensor([[[ 1.1641],
         [ 1.1641],
         [ 1.1641],
         ...,
         [-0.9102],
         [ 0.1396],
         [ 1.2578]],

        [[ 0.0115],
         [ 0.0115],
         [ 0.0115],
         ...,
         [-1.3438],
         [ 0.5430],
         [ 1.5391]],

        [[ 0.9102],
         [ 0.9102],
         [ 0.9102],
         ...,
         [-0.1934],
         [ 1.6797],
         [ 1.5703]],

        ...,

        [[ 2.2500],
         [ 2.2500],
         [ 2.2500],
         ...,
         [ 0.0121],
         [-2.6406],
         [-1.6875]],

        [[ 0.4258],
         [ 0.4258],
         [ 0.4258],
         ...,
         [-1.3516],
         [ 0.3809],
         [ 0.9922]],

        [[ 0.0767],
         [ 0.0767],
         [ 0.0767],
         ...,
         [ 1.3906],
         [ 0.9609],
         [-1.6406]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 1.2578,  1.5391,  1.5703,  1.1797,  0.8398, -1.3203, -0.2109, -2.3125,
        -0.5820,  0.2520, -1.2578, -0.5430, -1.0781, -1.6875,  0.9922, -1.6406],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ..., 32,  0,  0],
        ...,
        [ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ...,  0,  0,  0]], device='cuda:3') input_ids
tensor([[[-0.1396],
         [-0.1396],
         [-0.1396],
         ...,
         [ 0.4121],
         [ 0.4121],
         [ 0.4121]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 1.9688],
         [ 1.9688],
         [ 1.9688]],

        [[ 0.4746],
         [ 0.4746],
         [ 0.4746],
         ...,
         [ 5.8125],
         [ 5.8125],
         [ 5.8125]],

        ...,

        [[-0.3496],
         [-0.3496],
         [-0.3496],
         ...,
         [ 1.1484],
         [ 1.1484],
         [ 1.1484]],

        [[ 0.8359],
         [ 0.2969],
         [-1.2031],
         ...,
         [ 4.8125],
         [ 4.8125],
         [ 4.8125]],

        [[-1.9375],
         [-1.9375],
         [-1.9375],
         ...,
         [ 0.0884],
         [ 0.0884],
         [ 0.0884]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 0.1797,  1.9844,  5.5312,  1.3750,  3.4531,  1.0391,  3.5312,  2.6719,
         5.2812,  2.7188, -0.2520,  2.8281,  2.3906,  1.1484,  4.6562,  0.2754],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([539, 551, 535, 532, 529, 532, 556, 539, 535, 540, 530, 536, 537, 536,
        542, 545], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([[[ 1.4141],
         [ 1.4141],
         [ 1.4141],
         ...,
         [ 4.0000],
         [ 4.0000],
         [ 4.0000]],

        [[ 1.5547],
         [ 1.5547],
         [ 1.5547],
         ...,
         [ 4.3750],
         [ 4.3750],
         [ 4.3750]],

        [[ 0.2275],
         [ 0.2275],
         [ 0.2275],
         ...,
         [ 1.6562],
         [ 1.6562],
         [ 1.6562]],

        ...,

        [[ 1.0312],
         [ 1.0312],
         [ 1.0312],
         ...,
         [ 1.5547],
         [ 1.5547],
         [ 1.5547]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 4.6250],
         [ 4.6250],
         [ 4.6250]],

        [[ 0.0791],
         [ 0.0791],
         [ 0.0791],
         ...,
         [ 3.6094],
         [ 3.6094],
         [ 3.6094]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([4.0312, 4.2188, 1.6016, 4.1875, 0.2275, 1.1641, 2.8125, 1.0938, 1.3750,
        2.6562, 5.9688, 5.4375, 5.0625, 1.5625, 4.5000, 3.4688],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([534, 534, 544, 536, 539, 528, 534, 546, 539, 534, 559, 529, 548, 542,
        532, 533], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
torch.Size([16, 507]) query shape line 036
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
tensor([[[ 0.8516],
         [ 0.8516],
         [ 0.8516],
         ...,
         [ 1.9688],
         [ 1.9688],
         [ 1.9688]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 1.3047],
         [ 1.3047],
         [ 1.3047]],

        [[-2.0156],
         [-2.0156],
         [-2.0156],
         ...,
         [ 1.4688],
         [ 1.4688],
         [ 1.4688]],

        ...,

        [[ 0.0532],
         [ 0.0532],
         [ 0.0532],
         ...,
         [ 0.9648],
         [ 0.9648],
         [ 0.9648]],

        [[ 0.8242],
         [ 0.8242],
         [ 0.8242],
         ...,
         [ 4.0938],
         [ 4.0938],
         [ 4.0938]],

        [[ 1.4688],
         [ 1.4688],
         [ 1.4688],
         ...,
         [ 3.6406],
         [ 3.6406],
         [ 3.6406]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([1.8047, 1.3984, 1.4844, 1.6562, 1.0391, 3.3438, 6.8438, 2.1250, 0.5195,
        2.1562, 4.0000, 0.8516, 2.8594, 0.8594, 3.9688, 3.4219],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([541, 532, 539, 560, 544, 540, 552, 544, 538, 544, 545, 541, 564, 540,
        545, 531], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
torch.Size([16, 512]) query shape line 036
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
tensor([[[-0.9961],
         [-0.9961],
         [-0.9961],
         ...,
         [-0.3926],
         [-0.3926],
         [-0.3926]],

        [[-0.1074],
         [-0.1074],
         [-0.1074],
         ...,
         [ 3.0312],
         [ 3.0312],
         [ 3.0312]],

        [[-0.1875],
         [-0.1875],
         [-0.1875],
         ...,
         [ 1.3594],
         [ 1.3594],
         [ 1.3594]],

        ...,

        [[-0.7578],
         [-0.7578],
         [-0.7578],
         ...,
         [ 2.2812],
         [ 2.2812],
         [ 2.2812]],

        [[-0.1406],
         [-0.1406],
         [-0.1406],
         ...,
         [ 3.3281],
         [ 3.3281],
         [ 3.3281]],

        [[-0.3613],
         [-0.3613],
         [-0.3613],
         ...,
         [ 0.8867],
         [ 0.8867],
         [ 0.8867]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([[[ 1.3281],
         [ 1.3281],
         [ 1.3281],
         ...,
         [ 1.9141],
         [ 1.9141],
         [ 1.9141]],

        [[-1.5547],
         [-1.5547],
         [-1.5547],
         ...,
         [ 0.4336],
         [ 0.4336],
         [ 0.4336]],

        [[-1.5391],
         [-1.5391],
         [-1.5391],
         ...,
         [ 0.8477],
         [ 0.8477],
         [ 0.8477]],

        ...,

        [[ 0.0413],
         [ 0.0413],
         [ 0.0413],
         ...,
         [ 1.6719],
         [ 1.6719],
         [ 1.6719]],

        [[ 0.9961],
         [ 0.9961],
         [ 0.9961],
         ...,
         [ 4.2500],
         [ 4.2500],
         [ 4.2500]],

        [[ 2.5781],
         [ 2.5781],
         [ 2.5781],
         ...,
         [ 5.9375],
         [ 5.9375],
         [ 5.9375]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([-0.1904,  2.8125,  1.1328,  3.3750,  2.7031,  2.8906,  4.6562,  2.0938,
         2.5469, -0.9258,  0.0623,  3.2031,  5.2500,  2.1250,  3.2969,  0.6992],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([514, 516, 518, 528, 510, 517, 512, 511, 508, 514, 513, 518, 524, 519,
        515, 512], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
torch.Size([16, 488]) query shape line 036
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488tensor([1.8672, 0.4180, 0.9375, 1.7422, 2.7812, 2.6250, 4.0938, 1.1250, 0.3574,
        1.8047, 2.2188, 2.2812, 2.4219, 1.5312, 4.0625, 5.7812],
       device='cuda:0', dtype=torch.bfloat16) 
query line 308


 sequence_lengths, 488  query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
tensor([527, 526, 531, 531, 534, 544, 532, 533, 532, 530, 522, 526, 530, 528,
        532, 532], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
torch.Size([16, 500]) query shape line 036
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
tensor([[[-0.9688],
         [-0.9688],
         [-0.9688],
         ...,
         [-2.1875],
         [ 0.6797],
         [ 0.4473]],

        [[ 0.7227],
         [ 0.7227],
         [ 0.7227],
         ...,
         [-2.4844],
         [-1.1875],
         [-2.6719]],

        [[ 0.5664],
         [ 0.5664],
         [ 0.5664],
         ...,
         [ 0.6367],
         [ 2.8125],
         [ 1.8438]],

        ...,

        [[-0.7188],
         [-0.7188],
         [-0.7188],
         ...,
         [-2.2500],
         [-4.8125],
         [-1.9297]],

        [[-0.7422],
         [-0.7422],
         [-0.7422],
         ...,
         [ 0.5703],
         [-0.0845],
         [ 1.1484]],

        [[-0.5859],
         [-0.5859],
         [-0.5859],
         ...,
         [-3.4844],
         [-0.0840],
         [-0.6562]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([ 0.4473, -2.6719,  1.8438, -1.2500, -2.7969,  1.3672, -0.3379,  3.1250,
        -0.9805, -4.0625, -4.6250, -0.3750,  4.3438, -1.9297,  1.1484, -0.6562],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558,
        558, 558], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:4') input_ids
tensor([[[ 0.2031],
         [ 0.2031],
         [ 0.2031],
         ...,
         [ 2.0938],
         [ 2.0938],
         [ 2.0938]],

        [[ 0.5547],
         [ 0.5547],
         [ 0.5547],
         ...,
         [ 1.6406],
         [ 1.6406],
         [ 1.6406]],

        [[ 0.9414],
         [ 0.9414],
         [ 0.9414],
         ...,
         [-0.1934],
         [ 1.6797],
         [ 1.7734]],

        ...,

        [[ 1.7344],
         [ 1.7344],
         [ 1.7344],
         ...,
         [ 3.6875],
         [ 3.6875],
         [ 3.6875]],

        [[-0.3652],
         [-0.3652],
         [-0.3652],
         ...,
         [ 0.4121],
         [ 0.4121],
         [ 0.4121]],

        [[-0.4355],
         [-0.4355],
         [-0.4355],
         ...,
         [ 0.6016],
         [ 0.6016],
         [ 0.6016]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 1.9844,  1.5859,  1.6797,  0.5234, -0.5625,  3.0469, -0.3242,  0.8672,
         2.0156,  3.8750, -1.2031,  2.1562,  3.1094,  3.5938,  0.2451,  0.5859],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([536, 548, 559, 534, 550, 539, 535, 536, 534, 548, 545, 529, 544, 535,
        536, 533], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([[[-1.1719],
         [-1.1719],
         [-1.1719],
         ...,
         [-0.0737],
         [-0.0737],
         [-0.0737]],

        [[-0.1045],
         [-0.1045],
         [-0.1045],
         ...,
         [ 0.3516],
         [ 0.3516],
         [ 0.3516]],

        [[ 0.3711],
         [ 0.3711],
         [ 0.3711],
         ...,
         [ 2.5938],
         [ 2.5938],
         [ 2.5938]],

        ...,

        [[ 0.6523],
         [ 0.6523],
         [ 0.6523],
         ...,
         [ 1.9062],
         [ 1.9062],
         [ 1.9062]],

        [[ 0.2559],
         [ 0.2559],
         [ 0.2559],
         ...,
         [ 0.5469],
         [ 0.5469],
         [ 0.5469]],

        [[-1.4609],
         [-1.4609],
         [-1.4609],
         ...,
         [-0.3887],
         [-0.3887],
         [-0.3887]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([-0.3047,  0.3184,  2.5938,  0.5547,  1.6094,  0.4785,  3.3125,  5.2500,
         2.4375,  2.1406,  4.1250,  2.9062,  4.3750,  2.1094,  0.5508, -0.6836],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([538, 538, 536, 531, 549, 543, 536, 537, 542, 535, 535, 538, 539, 529,
        534, 529], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
torch.Size([16, 506]) query shape line 036
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
tensor([[   0,    0,    0,  ..., 2793, 1016,  643],
        [   0,    0,    0,  ..., 3226,  533,  891],
        [   0,    0,    0,  ...,  327,  594, 4541],
        ...,
        [   0,    0,    0,  ...,  327,  326,   15],
        [   0,    0,    0,  ...,  436,   15,  187],
        [   0,    0,    0,  ...,   15,  309,  588]], device='cuda:2') input_ids
tensor([[[ 2.3281],
         [ 2.3281],
         [ 2.3281],
         ...,
         [ 1.3984],
         [ 0.2451],
         [-0.3848]],

        [[ 0.9531],
         [ 0.9531],
         [ 0.9531],
         ...,
         [ 1.6719],
         [ 0.9609],
         [-1.1641]],

        [[ 0.5742],
         [ 0.5742],
         [ 0.5742],
         ...,
         [ 0.8633],
         [-1.1328],
         [ 4.1875]],

        ...,

        [[ 1.0703],
         [ 1.0703],
         [ 1.0703],
         ...,
         [ 3.6406],
         [ 1.7500],
         [-0.3047]],

        [[-0.5781],
         [-0.5781],
         [-0.5781],
         ...,
         [-1.7422],
         [-0.2197],
         [-0.2383]],

        [[ 1.1016],
         [ 1.1016],
         [ 1.1016],
         ...,
         [ 0.4219],
         [-2.6250],
         [-0.9414]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([-0.3848, -1.1641,  4.1875, -0.2695,  0.1128, -1.6953, -2.2500, -2.7812,
         5.3125, -0.3535,  0.5898, -0.5703, -1.6562, -0.3047, -0.2383, -0.9414],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562,
        562, 562], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,  327,  594, 4541],
        ...,
        [   0,    0,    0,  ...,  327,  326,   15],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:2') input_ids
tensor([[    0,     0,     0,  ...,   690,  7535,   327],
        [    0,     0,     0,  ...,   387,   247,  2957],
        [    0,     0,     0,  ...,   327,   352,    15],
        ...,
        [    0,     0,     0,  ...,  1175,  8183, 25325],
        [    0,     0,     0,  ...,   667,   273,   841],
        [    0,     0,     0,  ...,    32,   187,   187]], device='cuda:5') input_ids
tensor([[    0,     0,     0,  ...,   347,   619, 19609],
        [    0,     0,     0,  ...,   320,   247, 33702],
        [    0,     0,     0,  ...,  2970, 46745,    13],
        ...,
        [    0,     0,     0,  ...,  6110,    15,     0],
        [    0,     0,     0,  ...,   275,   253,  3054],
        [    0,     0,     0,  ...,   642,  2934,   436]], device='cuda:1') input_ids
tensor([[    0,     0,     0,  ...,   522, 12341,   366],
        [    0,     0,     0,  ...,   717,   417, 17755],
        [    0,     0,     0,  ...,   456,   327,   619],
        ...,
        [    0,     0,     0,  ...,  1353,  1469,   281],
        [    0,     0,     0,  ...,   452,  1633,   281],
        [    0,     0,     0,  ...,   309,   858,   690]], device='cuda:0') input_ids
tensor([[    0,     0,     0,  ...,    10,   452,   644],
        [    0,     0,     0,  ..., 18104,    15,   361],
        [    0,     0,     0,  ...,   619,  1495,    15],
        ...,
        [    0,     0,     0,  ..., 25172,   521,  6617],
        [    0,     0,     0,  ...,  2513,   436,   253],
        [    0,     0,     0,  ...,    15,  1737,   513]], device='cuda:7') input_ids
tensor([[    0,     0,     0,  ..., 17640,   401, 36170],
        [    0,     0,     0,  ...,   187,   187, 14135],
        [    0,     0,     0,  ...,   247,   637,   665],
        ...,
        [    0,     0,     0,  ..., 28542, 22273,   544],
        [    0,     0,     0,  ...,   717,  2819,   323],
        [    0,     0,     0,  ...,    15,   309,  7270]], device='cuda:6') input_ids
tensor([[    0,     0,     0,  ...,   891,  1353,   417],
        [    0,     0,     0,  ...,   564,  5053,   342],
        [    0,     0,     0,  ...,    15,   309,   452],
        ...,
        [    0,     0,     0,  ...,   479,   390,   816],
        [    0,     0,     0,  ...,   556,   247,  5884],
        [    0,     0,     0,  ..., 33408,   285,   309]], device='cuda:3') input_ids
tensor([[[ 2.8281],
         [ 2.8281],
         [ 2.8281],
         ...,
         [ 5.8125],
         [ 5.8125],
         [ 5.8125]],

        [[ 0.6797],
         [ 0.6797],
         [ 0.6797],
         ...,
         [ 2.5156],
         [ 2.5156],
         [ 2.5156]],

        [[ 0.5742],
         [ 0.5742],
         [ 0.5742],
         ...,
         [ 0.8633],
         [-1.1328],
         [ 4.1875]],

        ...,

        [[ 1.0703],
         [ 1.0703],
         [ 1.0703],
         ...,
         [ 3.6406],
         [ 1.7500],
         [-0.3047]],

        [[ 0.9766],
         [ 0.9766],
         [ 0.9766],
         ...,
         [ 2.4062],
         [ 2.4062],
         [ 2.4062]],

        [[ 2.0000],
         [ 2.0000],
         [ 2.0000],
         ...,
         [ 4.0000],
         [ 4.0000],
         [ 4.0000]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([ 5.5625,  2.2969,  4.1875,  3.2500, -0.2012, -1.1328,  5.1250,  3.8750,
         5.3125,  2.2500,  2.3750,  1.4766,  1.9766, -0.3047,  2.2500,  3.7812],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([546, 538, 562, 540, 547, 536, 545, 539, 562, 534, 545, 547, 535, 562,
        539, 532], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
torch.Size([16, 510]) query shape line 036
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
tensor([[[ 0.2354],
         [ 0.2354],
         [ 0.2354],
         ...,
         [-1.2031],
         [ 0.1494],
         [-1.3516]],

        [[ 1.2812],
         [ 1.2812],
         [ 1.2812],
         ...,
         [-1.9766],
         [-2.7188],
         [ 1.3203]],

        [[ 1.5859],
         [ 1.5859],
         [ 1.5859],
         ...,
         [ 1.7109],
         [ 4.6562],
         [ 4.1875]],

        ...,

        [[-1.3516],
         [-1.3516],
         [-1.3516],
         ...,
         [-0.6719],
         [-2.0469],
         [-1.3438]],

        [[ 2.7656],
         [ 2.7656],
         [ 2.7656],
         ...,
         [ 0.5352],
         [ 1.4688],
         [ 1.0234]],

        [[ 0.5391],
         [ 0.5391],
         [ 0.5391],
         ...,
         [ 0.7227],
         [ 3.7031],
         [ 1.9688]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-1.3516,  1.3203,  4.1875, -0.8398, -1.0859, -1.7500,  0.8828, -1.3125,
         1.0000,  2.6094,  0.9336,  0.3320, -3.7031, -1.3438,  1.0234,  1.9688],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:5') input_ids
tensor([[[ 0.5156],
         [ 0.5156],
         [ 0.5156],
         ...,
         [-1.5859],
         [-1.8438],
         [-0.9961]],

        [[ 0.3242],
         [ 0.3242],
         [ 0.3242],
         ...,
         [-4.2812],
         [-5.0000],
         [-3.6250]],

        [[-0.0991],
         [-0.0991],
         [-0.0991],
         ...,
         [-2.4375],
         [-0.3281],
         [-1.2266]],

        ...,

        [[ 2.0469],
         [ 2.0469],
         [ 2.0469],
         ...,
         [ 1.6406],
         [ 2.3906],
         [ 3.6719]],

        [[ 2.1094],
         [ 2.1094],
         [ 2.1094],
         ...,
         [ 4.6562],
         [ 3.0781],
         [ 4.4062]],

        [[ 1.4141],
         [ 1.4141],
         [ 1.4141],
         ...,
         [-0.9727],
         [-3.1875],
         [-3.3281]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([-0.9961, -3.6250, -1.2266, -2.0156, -2.6406,  1.2656, -0.5117,  0.9805,
         2.5156, -0.8672,  0.2910, -1.6328,  0.3242,  3.6719,  4.4062, -3.3281],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559,
        559, 559], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        ...,
        [   0,    0,    0,  ..., 6110,   15,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:1') input_ids
tensor([[[ 1.6094],
         [ 1.6094],
         [ 1.6094],
         ...,
         [ 1.7266],
         [ 1.9844],
         [ 1.8906]],

        [[ 0.1426],
         [ 0.1426],
         [ 0.1426],
         ...,
         [-2.3750],
         [-1.8047],
         [-0.9297]],

        [[ 0.9844],
         [ 0.9844],
         [ 0.9844],
         ...,
         [-2.2188],
         [-2.1719],
         [-3.4219]],

        ...,

        [[-1.9844],
         [-1.9844],
         [-1.9844],
         ...,
         [-2.0781],
         [ 1.7031],
         [-0.3809]],

        [[ 0.6250],
         [ 0.6250],
         [ 0.6250],
         ...,
         [-0.4844],
         [ 0.4160],
         [-0.0322]],

        [[ 1.6562],
         [ 1.6562],
         [ 1.6562],
         ...,
         [-1.2422],
         [-1.0625],
         [-2.0938]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([ 1.8906, -0.9297, -3.4219,  3.0625, -0.8398, -3.3594,  2.4375,  3.9219,
         1.5391, -2.9062, -1.3828,  0.5547, -3.9062, -0.3809, -0.0322, -2.0938],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552,
        552, 552], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
tensor([[[ 2.1250],
         [ 2.1250],
         [ 2.1250],
         ...,
         [ 2.2969],
         [ 1.2344],
         [ 1.7891]],

        [[-0.9336],
         [-0.9336],
         [-0.9336],
         ...,
         [ 2.9531],
         [ 1.3906],
         [ 1.0938]],

        [[ 1.1719],
         [ 1.1719],
         [ 1.1719],
         ...,
         [-0.9141],
         [ 0.5977],
         [-2.0156]],

        ...,

        [[ 0.9297],
         [ 0.9297],
         [ 0.9297],
         ...,
         [-0.9883],
         [-0.8125],
         [ 0.4043]],

        [[ 3.3906],
         [ 3.3906],
         [ 3.3906],
         ...,
         [ 3.2344],
         [ 2.7188],
         [ 4.7812]],

        [[ 0.9570],
         [ 0.9570],
         [ 0.9570],
         ...,
         [ 1.3828],
         [-0.2812],
         [-2.5469]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') input_ids
tensor([ 1.7891,  1.0938, -2.0156, -4.0938,  3.0312, -1.4297,  4.1875, -2.9062,
        -4.1250,  0.5508,  0.1309, -3.7188,  1.8906,  0.4043,  4.7812, -2.5469],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540,
        540, 540], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


tensor([[[ 1.1016],
         [ 1.1016],
         [ 1.1016],
         ...,
         [-0.8555],
         [-1.9453],
         [-3.2031]],

        [[ 1.9062],
         [ 1.9062],
         [ 1.9062],
         ...,
         [ 0.2100],
         [ 1.4766],
         [ 1.4766]],

        [[-1.5547],
         [-1.5547],
         [-1.5547],
         ...,
         [-2.7812],
         [-2.8906],
         [-3.2344]],

        ...,

        [[ 0.0101],
         [ 0.0101],
         [ 0.0101],
         ...,
         [ 2.4375],
         [ 2.4531],
         [ 2.7812]],

        [[-0.0605],
         [-0.0605],
         [-0.0605],
         ...,
         [ 1.7734],
         [-1.0312],
         [ 1.9531]],

        [[ 1.5625],
         [ 1.5625],
         [ 1.5625],
         ...,
         [ 4.3438],
         [ 2.1875],
         [ 1.5234]]], device='cuda:7', dtype=torch.bfloat16)torch.Size([16])

torch.Size([16, 565, 1]) reward_logits
tensor([-3.2031,  1.4766, -3.2344,  3.2812, -0.1465,  0.5820,  3.0469, -1.8984,
        -3.3594, -0.4961, -1.4062,  0.4297, -2.8125,  2.7812,  1.9531,  1.5234],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564,
        564, 564], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:6') input_ids
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:7') input_ids
tensor([[[-1.7734],
         [-1.7734],
         [-1.7734],
         ...,
         [-4.9375],
         [-5.0938],
         [-5.6875]],

        [[ 0.5195],
         [ 0.5195],
         [ 0.5195],
         ...,
         [-0.9180],
         [ 0.4238],
         [-0.0645]],

        [[-0.4355],
         [-0.4355],
         [-0.4355],
         ...,
         [-1.0234],
         [-3.5938],
         [-2.0781]],

        ...,

        [[ 1.8672],
         [ 1.8672],
         [ 1.8672],
         ...,
         [ 1.5781],
         [ 0.4043],
         [-0.5078]],

        [[ 0.4395],
         [ 0.4395],
         [ 0.4395],
         ...,
         [ 1.6250],
         [ 0.8477],
         [ 1.2891]],

        [[ 0.1318],
         [ 0.1318],
         [ 0.1318],
         ...,
         [ 1.4219],
         [ 0.2930],
         [-1.5234]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-5.6875, -0.0645, -2.0781,  2.8594, -1.6094, -1.4922,  4.0938, -1.6641,
         0.3945, -3.0312, -1.9688, -2.6562, -0.1719, -0.5078,  1.2891, -1.5234],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:3') input_ids
tensor([[    0,     0,     0,  ..., 19609,   556,   327],
        [    0,     0,     0,  ..., 10428,   187,   187],
        [    0,     0,     0,  ...,   436,    15,     0],
        ...,
        [    0,     0,     0,  ...,   844,  1097,   497],
        [    0,     0,     0,  ...,  4766,   273,   253],
        [    0,     0,     0,  ...,   344, 11121,   309]], device='cuda:4') input_ids
tensor([[[ 0.8906],
         [ 0.8906],
         [ 0.8906],
         ...,
         [ 3.8438],
         [ 3.8438],
         [ 3.8438]],

        [[ 0.7852],
         [ 0.7852],
         [ 0.7852],
         ...,
         [ 1.7969],
         [ 1.7969],
         [ 1.7969]],

        [[ 0.9727],
         [ 0.9727],
         [ 0.9727],
         ...,
         [ 5.2500],
         [ 5.2500],
         [ 5.2500]],

        ...,

        [[-0.0491],
         [-0.0491],
         [-0.0491],
         ...,
         [ 2.5156],
         [ 2.5156],
         [ 2.5156]],

        [[ 3.0156],
         [ 3.0156],
         [ 3.0156],
         ...,
         [ 4.6562],
         [ 4.6562],
         [ 4.6562]],

        [[-0.2637],
         [-0.2637],
         [-0.2637],
         ...,
         [ 2.0312],
         [ 2.0312],
         [ 2.0312]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([3.6250, 1.6953, 5.1875, 4.5312, 5.0625, 2.6719, 1.8828, 1.6094, 6.0938,
        2.2969, 2.0625, 2.2500, 3.3125, 2.7031, 4.6250, 1.9609],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([536, 537, 543, 534, 551, 540, 535, 530, 539, 544, 540, 552, 550, 540,
        551, 537], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([[[ 0.6016],
         [ 0.6016],
         [ 0.6016],
         ...,
         [ 4.1875],
         [ 4.1875],
         [ 4.1875]],

        [[ 2.1875],
         [ 2.1875],
         [ 2.1875],
         ...,
         [ 2.9062],
         [ 2.9062],
         [ 2.9062]],

        [[-0.1147],
         [-0.1147],
         [-0.1147],
         ...,
         [ 2.7500],
         [ 2.7500],
         [ 2.7500]],

        ...,

        [[ 2.0469],
         [ 2.0469],
         [ 2.0469],
         ...,
         [ 1.6406],
         [ 2.3906],
         [ 3.6719]],

        [[ 1.0859],
         [ 1.0859],
         [ 1.0859],
         ...,
         [ 5.6875],
         [ 5.6875],
         [ 5.6875]],

        [[ 0.9297],
         [ 0.9297],
         [ 0.9297],
         ...,
         [ 2.2812],
         [ 2.2812],
         [ 2.2812]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([ 3.8906,  3.1094,  2.6406,  1.8984,  0.5078,  5.1250, -0.5117,  0.4609,
         2.8125,  0.0718,  4.7812,  2.4375,  4.3438,  3.6719,  5.3750,  2.3438],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([537, 538, 534, 539, 536, 536, 559, 531, 537, 538, 541, 543, 545, 559,
        531, 535], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
torch.Size([16, 507]) query shape line 036
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
tensor([[[ 0.9648],
         [ 0.9648],
         [ 0.9648],
         ...,
         [ 2.9531],
         [ 2.9531],
         [ 2.9531]],

        [[-0.1064],
         [-0.1064],
         [-0.1064],
         ...,
         [ 1.0859],
         [ 1.0859],
         [ 1.0859]],

        [[ 0.4883],
         [ 0.4883],
         [ 0.4883],
         ...,
         [ 0.9648],
         [ 0.9648],
         [ 0.9648]],

        ...,

        [[-1.3672],
         [-1.3672],
         [-1.3672],
         ...,
         [ 0.7266],
         [ 0.7266],
         [ 0.7266]],

        [[ 0.7930],
         [ 0.7930],
         [ 0.7930],
         ...,
         [ 4.0625],
         [ 4.0625],
         [ 4.0625]],

        [[ 0.3242],
         [ 0.3242],
         [ 0.3242],
         ...,
         [ 3.2031],
         [ 3.2031],
         [ 3.2031]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([ 2.7656,  1.0234,  0.9961,  1.7578,  0.8828,  3.5781,  3.3594,  6.0938,
         3.3125, -0.0145,  5.8125,  2.0938,  2.1094,  0.7695,  4.0000,  2.9844],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([532, 535, 537, 525, 531, 528, 549, 537, 531, 540, 526, 533, 532, 530,
        532, 532], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
torch.Size([16, 500]) query shape line 036
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
tensor([[[ 2.0625],
         [ 2.0625],
         [ 2.0625],
         ...,
         [ 5.4688],
         [ 5.4688],
         [ 5.4688]],

        [[-0.8203],
         [-0.8203],
         [-0.8203],
         ...,
         [ 1.2656],
         [ 1.2656],
         [ 1.2656]],

        [[ 0.6367],
         [ 0.6367],
         [ 0.6367],
         ...,
         [ 4.2500],
         [ 4.2500],
         [ 4.2500]],

        ...,

        [[ 0.5195],
         [ 0.5195],
         [ 0.5195],
         ...,
         [ 1.9297],
         [ 1.9297],
         [ 1.9297]],

        [[ 1.4531],
         [ 1.4531],
         [ 1.4531],
         ...,
         [ 6.8125],
         [ 6.8125],
         [ 6.8125]],

        [[ 0.7344],
         [ 0.7344],
         [ 0.7344],
         ...,
         [ 3.5156],
         [ 3.5156],
         [ 3.5156]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([ 5.5000,  1.0234,  4.2500,  1.8047,  4.7188,  5.9062,  2.1094,  1.7266,
        -1.4141,  2.1094,  2.3281,  0.5469,  3.5000,  1.9219,  6.3125,  3.3281],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([535, 536, 525, 525, 515, 519, 521, 523, 530, 516, 520, 521, 518, 520,
        519, 523], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
torch.Size([16, 488]) query shape line 036
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
tensor([[[ 0.1816],
         [ 0.1816],
         [ 0.1816],
         ...,
         [ 2.0312],
         [ 2.0312],
         [ 2.0312]],

        [[ 0.4297],
         [ 0.4297],
         [ 0.4297],
         ...,
         [ 3.2031],
         [ 3.2031],
         [ 3.2031]],

        [[-1.0078],
         [-1.0078],
         [-1.0078],
         ...,
         [-2.6094],
         [-2.6094],
         [-2.6094]],

        ...,

        [[ 0.0728],
         [ 0.0728],
         [ 0.0728],
         ...,
         [ 1.1953],
         [ 1.1953],
         [ 1.1953]],

        [[-0.2520],
         [-0.2520],
         [-0.2520],
         ...,
         [ 3.5938],
         [ 3.5938],
         [ 3.5938]],

        [[ 0.0442],
         [ 0.0442],
         [ 0.0442],
         ...,
         [ 1.9375],
         [ 1.9375],
         [ 1.9375]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([ 1.9453,  3.0625, -2.5938,  3.2188,  2.6250,  2.2969, -0.3496, -0.1177,
         1.0938,  3.2031,  2.2188,  4.6875,  3.6875,  0.9219,  3.4844,  1.9219],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([542, 541, 534, 543, 551, 553, 544, 539, 549, 547, 544, 545, 536, 536,
        554, 536], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
torch.Size([16, 512]) query shape line 036
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
tensor([[[-1.4844],
         [-1.4844],
         [-1.4844],
         ...,
         [ 0.9570],
         [ 0.9570],
         [ 0.9570]],

        [[ 0.4590],
         [ 0.4590],
         [ 0.4590],
         ...,
         [ 2.3906],
         [ 2.3906],
         [ 2.3906]],

        [[ 0.1816],
         [ 0.1816],
         [ 0.1816],
         ...,
         [ 3.4375],
         [ 3.4375],
         [ 3.4375]],

        ...,

        [[ 0.5781],
         [ 0.5781],
         [ 0.5781],
         ...,
         [ 3.9844],
         [ 3.9844],
         [ 3.9844]],

        [[ 0.3965],
         [ 0.3965],
         [ 0.3965],
         ...,
         [ 4.9062],
         [ 4.9062],
         [ 4.9062]],

        [[ 0.2812],
         [ 0.2812],
         [ 0.2812],
         ...,
         [ 3.2188],
         [ 3.2188],
         [ 3.2188]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 0.8398,  2.3750,  3.2812,  4.1562,  1.6875,  3.2969,  3.2969,  0.5195,
         2.9844, -0.3984, -0.2656,  0.2422,  3.0625,  4.0938,  4.9062,  3.0312],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([539, 544, 534, 532, 538, 538, 534, 525, 550, 539, 530, 540, 552, 537,
        546, 546], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([[[ 1.7812],
         [ 1.7812],
         [ 1.7812],
         ...,
         [ 4.0938],
         [ 4.4062],
         [ 2.5781]],

        [[ 0.6367],
         [ 0.6367],
         [ 0.6367],
         ...,
         [ 1.7578],
         [ 0.9336],
         [ 2.4375]],

        [[-1.7188],
         [-1.7188],
         [-1.7188],
         ...,
         [-1.6328],
         [-3.2656],
         [-2.1562]],

        ...,

        [[-1.0625],
         [-1.0625],
         [-1.0625],
         ...,
         [-3.2656],
         [-4.1250],
         [-2.9688]],

        [[ 0.8164],
         [ 0.8164],
         [ 0.8164],
         ...,
         [ 0.1147],
         [-0.6133],
         [ 2.0938]],

        [[-0.3398],
         [-0.3398],
         [-0.3398],
         ...,
         [ 1.8438],
         [-0.4863],
         [ 1.4375]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([ 2.5781,  2.4375, -2.1562,  2.2031,  0.5977,  2.4531,  2.6250,  0.6250,
        -2.0000, -2.8750, -0.3340, -5.7812, -1.4375, -2.9688,  2.0938,  1.4375],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558,
        558, 558], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
tensor([[  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ..., 436,  15,   0],
        ...,
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0],
        [  0,   0,   0,  ...,   0,   0,   0]], device='cuda:4') input_ids
tensor([[[-0.0693],
         [-0.0693],
         [-0.0693],
         ...,
         [ 4.0312],
         [ 4.0312],
         [ 4.0312]],

        [[ 0.5859],
         [ 0.5859],
         [ 0.5859],
         ...,
         [ 2.6094],
         [ 2.6094],
         [ 2.6094]],

        [[-1.7188],
         [-1.7188],
         [-1.7188],
         ...,
         [-1.6328],
         [-3.2656],
         [-2.1562]],

        ...,

        [[-1.1406],
         [-1.1406],
         [-1.1406],
         ...,
         [ 1.7656],
         [ 1.7656],
         [ 1.7656]],

        [[ 1.0156],
         [ 1.0156],
         [ 1.0156],
         ...,
         [ 6.1875],
         [ 6.1875],
         [ 6.1875]],

        [[-0.5859],
         [-0.5859],
         [-0.5859],
         ...,
         [ 2.5000],
         [ 2.5000],
         [ 2.5000]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([ 3.8750,  2.7031, -2.1562,  1.0312, -0.7539,  3.5938,  5.4375,  2.4531,
         1.4219,  1.1250, -0.3340,  1.9844,  1.4609,  1.6016,  6.1562,  2.5312],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([534, 548, 558, 533, 536, 530, 535, 539, 540, 530, 558, 536, 536, 533,
        543, 539], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
torch.Size([16, 506]) query shape line 036
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
tensor([[    0,     0,     0,  ...,  2360,   275, 21438],
        [    0,     0,     0,  ...,  7507,  1866,   405],
        [    0,     0,     0,  ...,   434,  7408,   281],
        ...,
        [    0,     0,     0,  ..., 32724,    15,   309],
        [    0,     0,     0,  ...,    13,   574,  1620],
        [    0,     0,     0,  ...,   187, 14135,    28]], device='cuda:2') input_ids
tensor([[[ 2.6875],
         [ 2.6875],
         [ 2.6875],
         ...,
         [ 2.2969],
         [ 0.5391],
         [ 3.8594]],

        [[-0.1895],
         [-0.1895],
         [-0.1895],
         ...,
         [-1.1016],
         [-3.0156],
         [-0.2285]],

        [[ 0.4648],
         [ 0.4648],
         [ 0.4648],
         ...,
         [-0.2471],
         [-0.0830],
         [ 0.1191]],

        ...,

        [[-0.6328],
         [-0.6328],
         [-0.6328],
         ...,
         [-0.8750],
         [-2.3594],
         [-2.9844]],

        [[ 0.2334],
         [ 0.2334],
         [ 0.2334],
         ...,
         [-3.7656],
         [-5.0312],
         [-2.2812]],

        [[-0.4316],
         [-0.4316],
         [-0.4316],
         ...,
         [ 2.7344],
         [ 2.9219],
         [ 0.0219]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([ 3.8594, -0.2285,  0.1191,  1.6562, -2.3125,  0.2471, -4.5000, -1.7578,
        -1.2969, -1.9375, -2.1719,  0.9336,  5.8125, -2.9844, -2.2812,  0.0219],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562,
        562, 562], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:2') input_ids
tensor([[    0,     0,     0,  ...,   871,   752,   281],
        [    0,     0,     0,  ...,  6068,     2,  1737],
        [    0,     0,     0,  ...,  5599,   432,  2970],
        ...,
        [    0,     0,     0,  ...,   187,    42,  1053],
        [    0,     0,     0,  ...,  1353,   271, 26442],
        [    0,     0,     0,  ...,   533,   344,  4962]], device='cuda:5') input_ids
tensor([[    0,     0,     0,  ...,   619,  4481,    13],
        [    0,     0,     0,  ...,   309,  1833,   320],
        [    0,     0,     0,  ...,   651,   703,   755],
        ...,
        [    0,     0,     0,  ...,  1158,   352,   434],
        [    0,     0,     0,  ..., 13889,   275,   619],
        [    0,     0,     0,  ...,   690,  2793,   275]], device='cuda:1') input_ids
tensor([[   0,    0,    0,  ...,  253, 7408,  310],
        [   0,    0,    0,  ..., 6976,   13,  285],
        [   0,    0,    0,  ...,   27,  544, 4537],
        ...,
        [   0,    0,    0,  ...,  452, 1199, 2793],
        [   0,    0,    0,  ...,  849,  309, 3543],
        [   0,    0,    0,  ..., 4976,   27,  322]], device='cuda:0') input_ids
tensor([[    0,     0,     0,  ...,   344,  4391,   521],
        [    0,     0,     0,  ...,   247,  1355, 29493],
        [    0,     0,     0,  ...,   320,  5211,    16],
        ...,
        [    0,     0,     0,  ...,   187,     0,   187],
        [    0,     0,     0,  ...,    15,  1500,   310],
        [    0,     0,     0,  ...,   187,  1276,  6569]], device='cuda:6') input_ids
tensor([[    0,     0,     0,  ...,   281,   755,   689],
        [    0,     0,     0,  ...,  1642,    15,  9375],
        [    0,     0,     0,  ...,   533,   309,  1869],
        ...,
        [    0,     0,     0,  ...,   327,   253,   990],
        [    0,     0,     0,  ...,   247,  2372,  1361],
        [    0,     0,     0,  ..., 15661,    13,   247]], device='cuda:7') input_ids
tensor([[[ 1.1328],
         [ 1.1328],
         [ 1.1328],
         ...,
         [ 3.5156],
         [ 3.5156],
         [ 3.5156]],

        [[ 0.6016],
         [ 0.6016],
         [ 0.6016],
         ...,
         [ 3.9062],
         [ 3.9062],
         [ 3.9062]],

        [[ 1.0078],
         [ 1.0078],
         [ 1.0078],
         ...,
         [ 4.6875],
         [ 4.6875],
         [ 4.6875]],

        ...,

        [[-1.4766],
         [-1.4766],
         [-1.4766],
         ...,
         [-2.0156],
         [-2.0156],
         [-2.0156]],

        [[-0.2891],
         [-0.2891],
         [-0.2891],
         ...,
         [ 0.8125],
         [ 0.8125],
         [ 0.8125]],

        [[ 0.1001],
         [ 0.1001],
         [ 0.1001],
         ...,
         [ 2.5781],
         [ 2.5781],
         [ 2.5781]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([ 3.5312,  3.7031,  4.5312,  1.6016,  1.1719,  4.2188, -1.1016,  1.9375,
         1.0781, -0.6133, -1.1094,  3.0469,  4.3750, -1.8047,  0.6523,  2.6719],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([545, 534, 546, 543, 546, 538, 536, 550, 536, 542, 538, 546, 561, 546,
        542, 543], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
torch.Size([16, 510]) query shape line 036
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
510 query line 308
tensor([[[-0.0527],
         [-0.0527],
         [-0.0527],
         ...,
         [-2.7656],
         [-1.4609],
         [-3.6406]],

        [[ 0.2695],
         [ 0.2695],
         [ 0.2695],
         ...,
         [-3.0156],
         [-3.2188],
         [ 1.3359]],

        [[ 0.5117],
         [ 0.5117],
         [ 0.5117],
         ...,
         [ 0.5156],
         [ 1.4062],
         [-0.4727]],

        ...,

        [[-1.9531],
         [-1.9531],
         [-1.9531],
         ...,
         [-1.8125],
         [-5.0000],
         [-0.9141]],

        [[-0.8945],
         [-0.8945],
         [-0.8945],
         ...,
         [-3.3125],
         [ 1.4297],
         [ 0.1504]],

        [[-1.9766],
         [-1.9766],
         [-1.9766],
         ...,
         [-2.9375],
         [-4.2188],
         [-2.4062]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-3.6406,  1.3359, -0.4727,  1.1562, -2.2969,  0.8945,  2.3750,  3.4531,
        -3.5156,  3.3594, -1.8828,  0.0135,  0.0952, -0.9141,  0.1504, -2.4062],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:5') input_ids
tensor([[    0,     0,     0,  ...,    28,  4976,    27],
        [    0,     0,     0,  ..., 18738,   617,  2469],
        [    0,     0,     0,  ...,   971,   285,   878],
        ...,
        [    0,     0,     0,  ...,   533,   309,  1928],
        [    0,     0,     0,  ...,   327,   619, 22273],
        [    0,     0,     0,  ...,    27, 39714,    13]], device='cuda:3') input_ids
tensor([[[-0.8047],
         [-0.8047],
         [-0.8047],
         ...,
         [-3.9531],
         [-1.5547],
         [-2.0469]],

        [[-0.8164],
         [-0.8164],
         [-0.8164],
         ...,
         [-5.2812],
         [-5.6875],
         [-3.9844]],

        [[-0.6484],
         [-0.6484],
         [-0.6484],
         ...,
         [-4.1250],
         [-6.3125],
         [-2.8281]],

        ...,

        [[ 1.6250],
         [ 1.6250],
         [ 1.6250],
         ...,
         [ 0.2930],
         [-1.4766],
         [ 0.9180]],

        [[-2.2031],
         [-2.2031],
         [-2.2031],
         ...,
         [-3.7344],
         [-3.4531],
         [-2.8750]],

        [[ 0.9492],
         [ 0.9492],
         [ 0.9492],
         ...,
         [ 3.0781],
         [ 1.9219],
         [ 1.7422]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([-2.0469, -3.9844, -2.8281, -1.4844, -3.2500,  0.1777, -1.2656, -4.4062,
         2.7344,  1.4062,  2.8750,  1.8203,  1.8281,  0.9180, -2.8750,  1.7422],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559,
        559, 559], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1') input_ids
tensor([[[-0.9570],
         [-0.9570],
         [-0.9570],
         ...,
         [-1.6875],
         [-0.2773],
         [-2.3594]],

        [[-1.2188],
         [-1.2188],
         [-1.2188],
         ...,
         [-1.1641],
         [-3.4844],
         [-1.4219]],

        [[ 0.9688],
         [ 0.9688],
         [ 0.9688],
         ...,
         [-0.2490],
         [ 0.3457],
         [-2.3594]],

        ...,

        [[-0.6641],
         [-0.6641],
         [-0.6641],
         ...,
         [-2.8750],
         [-2.1094],
         [-2.2188]],

        [[-1.4531],
         [-1.4531],
         [-1.4531],
         ...,
         [-2.5938],
         [-3.8125],
         [-2.1094]],

        [[-0.0216],
         [-0.0216],
         [-0.0216],
         ...,
         [ 0.0540],
         [-0.3457],
         [ 0.8008]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([-2.3594, -1.4219, -2.3594,  0.2490, -0.3789,  1.7812, -0.4668,  0.1699,
         1.0312, -1.6719,  1.5938,  3.9531, -0.1836, -2.2188, -2.1094,  0.8008],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552,
        552, 552], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') input_ids
tensor([[[ 1.5078],
         [ 1.5078],
         [ 1.5078],
         ...,
         [ 5.4688],
         [ 5.4688],
         [ 5.4688]],

        [[ 1.3203],
         [ 1.3203],
         [ 1.3203],
         ...,
         [ 3.2344],
         [ 3.2344],
         [ 3.2344]],

        [[ 0.5352],
         [ 0.5352],
         [ 0.5352],
         ...,
         [ 2.2812],
         [ 2.2812],
         [ 2.2812]],

        ...,

        [[ 0.4102],
         [ 0.4102],
         [ 0.4102],
         ...,
         [ 2.2188],
         [ 2.2188],
         [ 2.2188]],

        [[-0.3613],
         [-0.3613],
         [-0.3613],
         ...,
         [ 1.8750],
         [ 1.8750],
         [ 1.8750]],

        [[-0.1465],
         [-0.1465],
         [-0.1465],
         ...,
         [-0.1582],
         [-0.1582],
         [-0.1582]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([[[ 0.1138],
         [ 0.1138],
         [ 0.1138],
         ...,
         [ 0.3652],
         [ 0.1099],
         [ 0.0442]],

        [[ 2.6250],
         [ 2.6250],
         [ 2.6250],
         ...,
         [ 1.2109],
         [ 0.7266],
         [ 3.2969]],

        [[-0.2832],
         [-0.2832],
         [-0.2832],
         ...,
         [-0.8086],
         [ 1.3047],
         [ 0.8750]],

        ...,

        [[ 2.1875],
         [ 2.1875],
         [ 2.1875],
         ...,
         [ 4.0000],
         [ 4.0000],
         [ 4.0000]],

        [[ 1.3516],
         [ 1.3516],
         [ 1.3516],
         ...,
         [ 3.2344],
         [ 1.1562],
         [-0.3105]],

        [[ 0.5508],
         [ 0.5508],
         [ 0.5508],
         ...,
         [ 2.5156],
         [ 1.8125],
         [ 1.3828]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([ 4.9062,  3.0156,  2.0938,  1.5469,  0.2451,  3.9062,  1.4141,  1.0391,
         0.0933,  1.1875,  1.3984,  0.4316,  4.1562,  2.1562,  1.5469, -0.0342],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([528, 537, 537, 542, 534, 541, 532, 540, 537, 533, 547, 555, 547, 545,
        551, 537], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 tensor([ 0.0442,  3.2969,  0.8750, -1.6250, -2.5938, -0.4199, -3.9375,  1.2422,
         0.6133,  0.5664, -2.4531,  0.1216,  0.0261,  4.0000, -0.3105,  1.3828],
       device='cuda:6', dtype=torch.bfloat16)query line 308



 sequence_lengths, 508  query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540,
        540, 540], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
tensor([[[-1.0234],
         [-1.0234],
         [-1.0234],
         ...,
         [-6.0625],
         [-0.5352],
         [-1.3438]],

        [[-0.5586],
         [-0.5586],
         [-0.5586],
         ...,
         [ 1.8203],
         [-1.3984],
         [-0.5508]],

        [[-0.7734],
         [-0.7734],
         [-0.7734],
         ...,
         [-0.9648],
         [-3.8750],
         [-0.6367]],

        ...,

        [[-0.3203],
         [-0.3203],
         [-0.3203],
         ...,
         [-0.5859],
         [-1.6172],
         [ 0.8789]],

        [[ 0.4102],
         [ 0.4102],
         [ 0.4102],
         ...,
         [-1.3906],
         [ 2.8750],
         [ 5.4375]],

        [[ 0.5820],
         [ 0.5820],
         [ 0.5820],
         ...,
         [ 0.7852],
         [ 1.2500],
         [-1.1250]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:6') input_ids
tensor([-1.3438, -0.5508, -0.6367, -6.3125, -3.1562,  0.2695, -3.0312, -1.8047,
        -1.2344, -2.9531,  2.8906, -2.6875, -0.7461,  0.8789,  5.4375, -1.1250],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564,
        564, 564], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:7') input_ids
tensor([[[-0.1055],
         [-0.1055],
         [-0.1055],
         ...,
         [-0.1289],
         [-0.9922],
         [-2.2812]],

        [[ 1.0391],
         [ 1.0391],
         [ 1.0391],
         ...,
         [ 1.4453],
         [ 0.2949],
         [ 2.6250]],

        [[-1.2109],
         [-1.2109],
         [-1.2109],
         ...,
         [-0.4160],
         [-0.7656],
         [-1.0156]],

        ...,

        [[ 0.0201],
         [ 0.0201],
         [ 0.0201],
         ...,
         [-1.7188],
         [-3.5312],
         [-1.5703]],

        [[ 0.3672],
         [ 0.3672],
         [ 0.3672],
         ...,
         [-1.3203],
         [-2.4062],
         [ 1.5078]],

        [[ 0.9648],
         [ 0.9648],
         [ 0.9648],
         ...,
         [-2.0938],
         [-2.2656],
         [-2.5312]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-2.2812,  2.6250, -1.0156,  0.0962,  0.5000, -1.1016, -0.6055, -4.0000,
        -0.0752, -0.9883, -1.4922, -0.1836, -2.8125, -1.5703,  1.5078, -2.5312],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:3') input_ids
tensor([[    0,     0,     0,  ...,   187,    53, 43561],
        [    0,     0,     0,  ...,   717,  8261,   342],
        [    0,     0,     0,  ...,   309,  1053,   626],
        ...,
        [    0,     0,     0,  ...,  1379,   247,  4839],
        [    0,     0,     0,  ...,  1728,   323,   253],
        [ 9242,  7941,  1703,  ...,   285,  1841,   403]], device='cuda:4') input_ids
tensor([[[-0.1357],
         [-0.1357],
         [-0.1357],
         ...,
         [ 0.7930],
         [ 0.7930],
         [ 0.7930]],

        [[ 0.3203],
         [ 0.3203],
         [ 0.3203],
         ...,
         [ 3.0625],
         [ 3.0625],
         [ 3.0625]],

        [[-0.4082],
         [-0.4082],
         [-0.4082],
         ...,
         [ 1.2734],
         [ 1.2734],
         [ 1.2734]],

        ...,

        [[ 0.9609],
         [ 0.9609],
         [ 0.9609],
         ...,
         [ 1.6719],
         [ 1.6719],
         [ 1.6719]],

        [[-1.7891],
         [-1.7891],
         [-1.7891],
         ...,
         [-0.6172],
         [-0.6172],
         [-0.6172]],

        [[ 0.7617],
         [ 0.7617],
         [ 0.7617],
         ...,
         [ 4.8125],
         [ 4.8125],
         [ 4.8125]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([ 0.7422,  3.0156,  1.1484,  1.6562,  2.2500,  2.5469,  3.1406,  4.5625,
         2.9219,  0.1992,  3.1719,  3.0156,  3.4375,  1.4844, -0.5195,  4.4062],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([544, 536, 544, 546, 542, 538, 549, 534, 534, 542, 534, 544, 534, 530,
        543, 540], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
torch.Size([16, 507]) query shape line 036
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
507 query line 308
tensor([[[-0.6016],
         [-0.6016],
         [-0.6016],
         ...,
         [ 0.8906],
         [ 0.8906],
         [ 0.8906]],

        [[-1.6250],
         [-1.6250],
         [-1.6250],
         ...,
         [ 0.1387],
         [ 0.1387],
         [ 0.1387]],

        [[ 1.1328],
         [ 1.1328],
         [ 1.1328],
         ...,
         [ 4.4375],
         [ 4.4375],
         [ 4.4375]],

        ...,

        [[ 0.0354],
         [ 0.0354],
         [ 0.0354],
         ...,
         [ 1.6484],
         [ 1.6484],
         [ 1.6484]],

        [[ 0.2969],
         [ 0.2969],
         [ 0.2969],
         ...,
         [ 0.0742],
         [ 0.0742],
         [ 0.0742]],

        [[-1.0078],
         [-1.0078],
         [-1.0078],
         ...,
         [ 3.4531],
         [ 3.4531],
         [ 3.4531]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([ 0.7539,  0.0698,  4.3750,  0.2490,  1.4297,  3.5625,  1.8750,  1.9688,
         3.1875,  1.2734,  1.5156,  6.4375,  1.4062,  1.6094, -0.0688,  3.4531],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([533, 525, 544, 552, 537, 534, 529, 527, 525, 537, 526, 528, 524, 533,
        523, 542], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
torch.Size([16, 500]) query shape line 036
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
500 query line 308
tensor([[[-0.0703],
         [-0.0703],
         [-0.0703],
         ...,
         [ 3.3438],
         [ 3.3438],
         [ 3.3438]],

        [[ 2.0625],
         [ 2.0625],
         [ 2.0625],
         ...,
         [ 5.2812],
         [ 5.2812],
         [ 5.2812]],

        [[ 0.2217],
         [ 0.2217],
         [ 0.2217],
         ...,
         [ 0.8164],
         [ 0.8164],
         [ 0.8164]],

        ...,

        [[ 1.1797],
         [ 1.1797],
         [ 1.1797],
         ...,
         [ 4.6875],
         [ 4.6875],
         [ 4.6875]],

        [[ 0.6211],
         [ 0.6211],
         [ 0.6211],
         ...,
         [ 3.3281],
         [ 3.3281],
         [ 3.3281]],

        [[ 0.1279],
         [ 0.1279],
         [ 0.1279],
         ...,
         [ 1.6328],
         [ 1.6328],
         [ 1.6328]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([3.0781, 5.0938, 0.6250, 1.3984, 3.0312, 1.6172, 4.5625, 2.6094, 1.8672,
        3.1094, 2.3125, 1.1484, 2.8438, 4.4062, 3.1875, 1.4141],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([521, 528, 514, 513, 526, 514, 517, 517, 507, 512, 515, 512, 535, 512,
        517, 509], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
torch.Size([16, 488]) query shape line 036
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
488 query line 308
tensor([[[ 0.0928],
         [ 0.0928],
         [ 0.0928],
         ...,
         [ 1.7031],
         [ 1.7031],
         [ 1.7031]],

        [[ 0.0645],
         [ 0.0645],
         [ 0.0645],
         ...,
         [ 0.2168],
         [ 0.2168],
         [ 0.2168]],

        [[-0.5586],
         [-0.5586],
         [-0.5586],
         ...,
         [ 1.2656],
         [ 1.2656],
         [ 1.2656]],

        ...,

        [[-0.0996],
         [-0.0996],
         [-0.0996],
         ...,
         [ 0.6055],
         [ 0.6055],
         [ 0.6055]],

        [[ 1.8984],
         [ 1.8984],
         [ 1.8984],
         ...,
         [ 6.2500],
         [ 6.2500],
         [ 6.2500]],

        [[ 0.8672],
         [ 0.8672],
         [ 0.8672],
         ...,
         [ 3.0625],
         [ 3.0625],
         [ 3.0625]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([ 1.6797,  0.0243,  1.0234,  3.0625, -0.2656,  1.3281,  2.7500,  0.9883,
         0.4590,  2.2812,  3.3125, -0.4062,  0.4648,  0.4941,  6.1250,  2.5781],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([540, 544, 547, 552, 546, 550, 547, 537, 538, 557, 550, 543, 539, 546,
        555, 544], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
torch.Size([16, 512]) query shape line 036
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
512 query line 308
tensor([[[-0.9102],
         [-0.9102],
         [-0.9102],
         ...,
         [ 1.4141],
         [ 1.4141],
         [ 1.4141]],

        [[ 0.4922],
         [ 0.4922],
         [ 0.4922],
         ...,
         [ 3.5156],
         [ 3.5156],
         [ 3.5156]],

        [[-1.2891],
         [-1.2891],
         [-1.2891],
         ...,
         [ 0.3711],
         [ 0.3711],
         [ 0.3711]],

        ...,

        [[-1.0469],
         [-1.0469],
         [-1.0469],
         ...,
         [ 1.7344],
         [ 1.7344],
         [ 1.7344]],

        [[-0.1035],
         [-0.1035],
         [-0.1035],
         ...,
         [ 3.8594],
         [ 3.8594],
         [ 3.8594]],

        [[ 0.5586],
         [ 0.5586],
         [ 0.5586],
         ...,
         [ 2.8906],
         [ 2.8906],
         [ 2.8906]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([1.2656, 3.1094, 0.3379, 3.8125, 5.0000, 0.0085, 3.2188, 3.7344, 1.8906,
        1.4531, 2.0469, 2.7812, 2.9844, 1.6406, 4.0312, 2.7500],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([553, 540, 533, 535, 532, 545, 552, 535, 540, 539, 536, 549, 552, 547,
        547, 553], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 508]) query shape line 036
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
508 query line 308
tensor([[[ 1.1875],
         [ 1.1875],
         [ 1.1875],
         ...,
         [ 3.0781],
         [ 2.0312],
         [-0.4688]],

        [[ 0.7109],
         [ 0.7109],
         [ 0.7109],
         ...,
         [-1.3438],
         [-0.6016],
         [-0.2734]],

        [[-0.9141],
         [-0.9141],
         [-0.9141],
         ...,
         [-2.5469],
         [ 0.0530],
         [-3.9062]],

        ...,

        [[ 0.2490],
         [ 0.2490],
         [ 0.2490],
         ...,
         [ 0.1182],
         [ 2.9375],
         [ 2.7344]],

        [[ 1.3984],
         [ 1.3984],
         [ 1.3984],
         ...,
         [ 0.2178],
         [ 2.1094],
         [ 2.6875]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [-0.6641],
         [ 0.6172],
         [-2.1250]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([-0.4688, -0.2734, -3.9062,  4.2812, -0.3027,  1.3438, -1.8672,  2.3438,
        -3.0156,  2.7969,  1.2266, -0.2852, -2.4062,  2.7344,  2.6875, -2.1250],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558,
        558, 558], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        ...,
        [   0,    0,    0,  ...,    0,    0,    0],
        [   0,    0,    0,  ...,    0,    0,    0],
        [9242, 7941, 1703,  ...,    0,    0,    0]], device='cuda:4') input_ids
tensor([[[ 1.2500],
         [ 1.2500],
         [ 1.2500],
         ...,
         [ 3.7188],
         [ 3.7188],
         [ 3.7188]],

        [[ 0.7422],
         [ 0.7422],
         [ 0.7422],
         ...,
         [ 3.2344],
         [ 3.2344],
         [ 3.2344]],

        [[-1.0234],
         [-1.0234],
         [-1.0234],
         ...,
         [ 1.2656],
         [ 1.2656],
         [ 1.2656]],

        ...,

        [[-0.0718],
         [-0.0718],
         [-0.0718],
         ...,
         [ 2.0781],
         [ 2.0781],
         [ 2.0781]],

        [[ 0.0540],
         [ 0.0540],
         [ 0.0540],
         ...,
         [ 1.3906],
         [ 1.3906],
         [ 1.3906]],

        [[ 0.2969],
         [-1.2031],
         [-1.9922],
         ...,
         [ 2.5625],
         [ 2.5625],
         [ 2.5625]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([3.6719, 2.9062, 1.2109, 4.5312, 2.4688, 2.7031, 1.6094, 3.1719, 0.6797,
        1.2812, 2.6250, 1.9609, 4.3125, 1.9453, 1.4062, 2.4688],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([553, 523, 534, 534, 539, 541, 539, 537, 537, 557, 532, 550, 534, 537,
        540, 534], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
torch.Size([16, 506]) query shape line 036
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
506 query line 308
tensor([[    0,     0,     0,  ...,    13,   309,  6704],
        [    0,     0,     0,  ...,   187, 14135,    28],
        [    0,     0,     0,  ...,   971,   281,   320],
        ...,
        [    0,     0,     0,  ...,  3153,  1735,   281],
        [    0,     0,     0,  ...,   309,   717,   247],
        [    0,     0,     0,  ...,   626,   871,   752]], device='cuda:2') input_ids
tensor([[[ 0.9531],
         [ 0.9531],
         [ 0.9531],
         ...,
         [ 1.1484],
         [-0.0430],
         [-0.1079]],

        [[-0.6914],
         [-0.6914],
         [-0.6914],
         ...,
         [ 2.4531],
         [ 2.8125],
         [ 0.8984]],

        [[ 0.1572],
         [ 0.1572],
         [ 0.1572],
         ...,
         [-2.5000],
         [-0.5273],
         [-1.5625]],

        ...,

        [[-1.1641],
         [-1.1641],
         [-1.1641],
         ...,
         [-1.9062],
         [-0.5586],
         [-2.9375]],

        [[-0.3965],
         [-0.3965],
         [-0.3965],
         ...,
         [-3.0781],
         [-3.7656],
         [-4.5000]],

        [[-0.8672],
         [-0.8672],
         [-0.8672],
         ...,
         [-4.5312],
         [-1.9062],
         [ 1.5781]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([-0.1079,  0.8984, -1.5625,  3.1406,  2.3438, -1.9844, -1.8906, -1.8984,
        -0.7148, -2.6250, -2.1875, -3.4062, -2.5312, -2.9375, -4.5000,  1.5781],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562, 562,
        562, 562], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
tensor([[   0,    0,    0,  ..., 1849,  644, 2366],
        [   0,    0,    0,  ...,  878,  281, 2028],
        [   0,    0,    0,  ...,   28, 4976,   27],
        ...,
        [   0,    0,    0,  ...,  247, 9484,  273],
        [   0,    0,    0,  ...,  273,  619,  673],
        [   0,    0,    0,  ...,  187,  187,   42]], device='cuda:5') input_ids
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:2') input_ids
tensor([[    0,     0,     0,  ...,   187,   187, 15743],
        [    0,     0,     0,  ...,   342,   479,    15],
        [    0,     0,     0,  ...,   776,  2675, 14240],
        ...,
        [    0,     0,     0,  ...,   187,    14,   380],
        [    0,     0,     0,  ...,  1563,   952,    27],
        [    0,     0,     0,  ..., 14008,  7250,   479]], device='cuda:1') input_ids
tensor([[    0,     0,     0,  ...,  8057,  4181,  2954],
        [    0,     0,     0,  ...,  8314, 40685, 11555],
        [    0,     0,     0,  ...,   247,  4564,   273],
        ...,
        [    0,     0,     0,  ...,    27,   309,  1353],
        [    0,     0,     0,  ...,   369,   816,  1146],
        [    0,     0,     0,  ...,   247,  1899,   281]], device='cuda:0') input_ids
tensor([[[ 1.0469],
         [ 1.0469],
         [ 1.0469],
         ...,
         [ 3.5625],
         [ 3.5625],
         [ 3.5625]],

        [[-0.5977],
         [-0.5977],
         [-0.5977],
         ...,
         [ 2.7031],
         [ 2.7031],
         [ 2.7031]],

        [[ 0.5703],
         [ 0.5703],
         [ 0.5703],
         ...,
         [ 3.2812],
         [ 3.2812],
         [ 3.2812]],

        ...,

        [[-1.0000],
         [-1.0000],
         [-1.0000],
         ...,
         [ 0.8320],
         [ 0.8320],
         [ 0.8320]],

        [[ 0.2754],
         [ 0.2754],
         [ 0.2754],
         ...,
         [ 4.0625],
         [ 4.0625],
         [ 4.0625]],

        [[-0.2773],
         [-0.2773],
         [-0.2773],
         ...,
         [ 1.7422],
         [ 1.7422],
         [ 1.7422]]], device='cuda:2', dtype=torch.bfloat16)
torch.Size([16, 563, 1]) reward_logits
tensor([ 3.3594,  2.5781,  2.9688,  2.7500,  2.5312,  1.6172,  2.5625,  3.4688,
         2.7031,  3.1250,  2.8906, -1.1719,  0.4746,  0.4902,  4.0000,  1.7109],
       device='cuda:2', dtype=torch.bfloat16)


 sequence_lengths,  tensor([536, 557, 537, 533, 538, 536, 537, 545, 552, 547, 549, 537, 541, 542,
        547, 554], device='cuda:2') torch.Size([16, 563, 1]) 510 line 971 


torch.Size([16])
tensor([[    0,     0,     0,  ...,   604,   309,   943],
        [    0,     0,     0,  ...,  3622,    13,   285],
        [    0,     0,     0,  ..., 50276,    42,   878],
        ...,
        [    0,     0,     0,  ...,   352,   434, 12759],
        [    0,     0,     0,  ...,  1048,  1307,  2954],
        [    0,     0,     0,  ...,  4976,    27, 21979]], device='cuda:6') input_ids
tensor([[[-1.8984],
         [-1.8984],
         [-1.8984],
         ...,
         [-1.8359],
         [-4.5000],
         [-1.6250]],

        [[ 0.4219],
         [ 0.4219],
         [ 0.4219],
         ...,
         [-1.7891],
         [-2.3125],
         [-1.4141]],

        [[ 1.8984],
         [ 1.8984],
         [ 1.8984],
         ...,
         [-0.0854],
         [ 1.5703],
         [-0.3984]],

        ...,

        [[-0.5039],
         [-0.5039],
         [-0.5039],
         ...,
         [-0.8945],
         [ 3.1250],
         [-0.0356]],

        [[ 0.8633],
         [ 0.8633],
         [ 0.8633],
         ...,
         [ 0.6562],
         [-0.6406],
         [ 2.2969]],

        [[ 1.0078],
         [ 1.0078],
         [ 1.0078],
         ...,
         [ 2.2812],
         [ 1.7422],
         [-2.9531]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-1.6250, -1.4141, -0.3984, -0.7109, -3.1562, -2.2031, -4.1250, -5.9375,
        -1.3047,  0.2227,  2.6875,  0.2051,  0.5938, -0.0356,  2.2969, -2.9531],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:5') input_ids
tensor([[    0,     0,     0,  ...,   619,  3331,   434],
        [    0,     0,     0,  ..., 13597,   323,   247],
        [    0,     0,     0,  ...,  5108,    32,   309],
        ...,
        [    0,     0,     0,  ...,  1053,   626,   871],
        [    0,     0,     0,  ...,   281,  5513,   752],
        [    0,     0,     0,  ...,   281,   513,    15]], device='cuda:7') input_ids
tensor([[    0,     0,     0,  ...,   309,  1053,   626],
        [    0,     0,     0,  ...,   253,   643,    15],
        [    0,     0,     0,  ...,  2564,    13,  1024],
        ...,
        [    0,     0,     0,  ...,  3543,   436,  1039],
        [    0,     0,     0,  ...,   187,   187, 14135],
        [    0,     0,     0,  ..., 14758,  8471,   273]], device='cuda:3') input_ids
tensor([[[ 0.3086],
         [ 0.3086],
         [ 0.3086],
         ...,
         [ 1.7578],
         [-0.3887],
         [-1.2969]],

        [[ 0.6445],
         [ 0.6445],
         [ 0.6445],
         ...,
         [ 1.0859],
         [ 2.7188],
         [ 1.9766]],

        [[ 0.3730],
         [ 0.3730],
         [ 0.3730],
         ...,
         [ 1.4062],
         [-0.3828],
         [-1.0547]],

        ...,

        [[ 0.3340],
         [ 0.3340],
         [ 0.3340],
         ...,
         [ 1.7578],
         [ 0.5977],
         [ 0.7383]],

        [[-0.0669],
         [-0.0669],
         [-0.0669],
         ...,
         [-0.2656],
         [ 0.2041],
         [-0.5234]],

        [[ 0.6719],
         [ 0.6719],
         [ 0.6719],
         ...,
         [ 0.2539],
         [-1.8750],
         [ 1.2344]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([-1.2969,  1.9766, -1.0547, -1.0391, -3.3281, -3.4062, -2.0781,  3.9062,
         0.4609, -2.9062,  0.0903,  2.6406, -0.4258,  0.7383, -0.5234,  1.2344],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([[[ 2.0469],
         [ 2.0469],
         [ 2.0469],
         ...,
         [-0.7695],
         [ 0.1396],
         [-0.0583]],

        [[ 3.8125],
         [ 3.8125],
         [ 3.8125],
         ...,
         [-2.7031],
         [-0.0835],
         [ 0.3379]],

        [[-0.1943],
         [-0.1943],
         [-0.1943],
         ...,
         [-2.5938],
         [ 0.7539],
         [-0.2793]],

        ...,

        [[-1.3594],
         [-1.3594],
         [-1.3594],
         ...,
         [-3.0312],
         [-4.6250],
         [-4.8750]],

        [[ 0.5312],
         [ 0.5312],
         [ 0.5312],
         ...,
         [-1.5547],
         [-1.3828],
         [-2.2031]],

        [[ 1.4844],
         [ 1.4844],
         [ 1.4844],
         ...,
         [-4.3125],
         [-1.7812],
         [-2.6719]]], device='cuda:0', dtype=torch.bfloat16)
tensor([559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559, 559,
        559, 559], device='cuda:1')torch.Size([16, 553, 1])  torch.Size([16, 560, 1])reward_logits 
507 line 971 


torch.Size([16])
tensor([-0.0583,  0.3379, -0.2793, -3.9062,  3.4219, -3.9688, -4.0938, -1.6328,
        -2.8750, -1.9219,  2.1250,  0.0623, -1.3281, -4.8750, -2.2031, -2.6719],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552, 552,
        552, 552], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:1') input_ids
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') input_ids
tensor([[[-1.5391],
         [-1.5391],
         [-1.5391],
         ...,
         [ 2.9844],
         [ 2.9844],
         [ 2.9844]],

        [[ 0.3711],
         [ 0.3711],
         [ 0.3711],
         ...,
         [ 1.8047],
         [ 1.8047],
         [ 1.8047]],

        [[ 1.7344],
         [ 1.7344],
         [ 1.7344],
         ...,
         [ 4.8125],
         [ 4.8125],
         [ 4.8125]],

        ...,

        [[-0.5898],
         [-0.5898],
         [-0.5898],
         ...,
         [ 0.3184],
         [ 0.3184],
         [ 0.3184]],

        [[-1.0078],
         [-1.0078],
         [-1.0078],
         ...,
         [-0.6406],
         [-0.6406],
         [-0.6406]],

        [[ 0.2246],
         [ 0.2246],
         [ 0.2246],
         ...,
         [ 1.5469],
         [ 1.5469],
         [ 1.5469]]], device='cuda:5', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 2.8906,  1.6953,  4.7812,  4.4375,  3.5625,  2.6875,  0.4355,  2.3594,
        -1.9688,  2.7031,  4.8438, -0.3105, -0.5508,  0.2031, -0.4590,  1.6406],
       device='cuda:5', dtype=torch.bfloat16)


 sequence_lengths,  tensor([540, 542, 553, 531, 532, 551, 535, 545, 548, 542, 542, 532, 541, 542,
        541, 549], device='cuda:5') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[[ 0.9258],
         [ 0.9258],
         [ 0.9258],
         ...,
         [ 0.2070],
         [-0.2969],
         [-0.1973]],

        [[ 2.2031],
         [ 2.2031],
         [ 2.2031],
         ...,
         [ 5.2188],
         [ 4.8125],
         [ 3.1406]],

        [[-1.9688],
         [-1.9688],
         [-1.9688],
         ...,
         [ 0.7695],
         [-2.3125],
         [ 0.2812]],

        ...,

        [[-0.3262],
         [-0.3262],
         [-0.3262],
         ...,
         [-0.7148],
         [-1.4688],
         [-5.8750]],

        [[-0.1338],
         [-0.1338],
         [-0.1338],
         ...,
         [-3.7031],
         [-2.6562],
         [ 0.1797]],

        [[ 0.6250],
         [ 0.6250],
         [ 0.6250],
         ...,
         [-1.0938],
         [ 0.2754],
         [-1.1172]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([-0.1973,  3.1406,  0.2812, -4.0000, -2.0156,  0.1797,  1.7188, -4.5938,
        -0.1738,  1.4844, -1.1953,  5.7812,  0.8945, -5.8750,  0.1797, -1.1172],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540,
        540, 540], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:6') input_ids
tensor([[[ 0.8828],
         [ 0.8828],
         [ 0.8828],
         ...,
         [ 1.4922],
         [ 3.2656],
         [ 2.1719]],

        [[ 1.2109],
         [ 1.2109],
         [ 1.2109],
         ...,
         [-0.4785],
         [ 0.2383],
         [-0.8789]],

        [[ 3.3125],
         [ 3.3125],
         [ 3.3125],
         ...,
         [ 3.5781],
         [ 3.8125],
         [ 2.7812]],

        ...,

        [[ 0.2139],
         [ 0.2139],
         [ 0.2139],
         ...,
         [ 3.3438],
         [-1.9609],
         [-1.3281]],

        [[ 0.1270],
         [ 0.1270],
         [ 0.1270],
         ...,
         [-4.5938],
         [-0.6289],
         [ 1.3281]],

        [[ 2.9531],
         [ 2.9531],
         [ 2.9531],
         ...,
         [ 2.9062],
         [ 6.0625],
         [ 5.8750]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([ 2.1719, -0.8789,  2.7812, -2.0312, -1.2969, -0.5898,  4.2812, -2.1875,
         1.4062,  0.7344, -1.6484, -1.8047,  0.4531, -1.3281,  1.3281,  5.8750],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564,
        564, 564], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:7') input_ids
tensor([[[ 0.4727],
         [ 0.4727],
         [ 0.4727],
         ...,
         [-4.2188],
         [ 0.5898],
         [-5.7812]],

        [[ 0.2539],
         [ 0.2539],
         [ 0.2539],
         ...,
         [-2.6094],
         [-2.7500],
         [-1.4609]],

        [[ 2.3125],
         [ 2.3125],
         [ 2.3125],
         ...,
         [ 3.5156],
         [ 4.1562],
         [ 2.9531]],

        ...,

        [[-1.3984],
         [-1.3984],
         [-1.3984],
         ...,
         [-5.6250],
         [-3.9219],
         [-2.2969]],

        [[ 2.2344],
         [ 2.2344],
         [ 2.2344],
         ...,
         [ 4.9688],
         [ 3.5000],
         [ 2.2344]],

        [[ 0.3125],
         [ 0.3125],
         [ 0.3125],
         ...,
         [-0.6719],
         [-3.1562],
         [-0.4941]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([-5.7812, -1.4609,  2.9531, -1.4531, -1.5703,  0.7148, -0.2363,  3.5469,
        -1.8828, -0.1738, -2.1875, -3.9688, -1.4766, -2.2969,  2.2344, -0.4941],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560,
        560, 560], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:3') input_ids
tensor([[    0,     0,     0,  ...,  1083,    15,   309],
        [    0,     0,     0,  ...,    84,   285,   611],
        [    0,     0,     0,  ...,  1158,   309,  1353],
        ...,
        [    0,     0,     0,  ...,  1500,   556,   642],
        [    0,     0,     0,  ..., 11527,   342, 23629],
        [    0,     0,     0,  ...,   417,  8958,   604]], device='cuda:4') input_ids
tensor([[[-0.8789],
         [-0.8789],
         [-0.8789],
         ...,
         [ 1.3438],
         [ 1.3438],
         [ 1.3438]],

        [[-0.1367],
         [-0.1367],
         [-0.1367],
         ...,
         [ 1.0469],
         [ 1.0469],
         [ 1.0469]],

        [[-0.0767],
         [-0.0767],
         [-0.0767],
         ...,
         [ 1.9219],
         [ 1.9219],
         [ 1.9219]],

        ...,

        [[ 0.2773],
         [ 0.2773],
         [ 0.2773],
         ...,
         [ 1.5078],
         [ 1.5078],
         [ 1.5078]],

        [[ 0.4844],
         [ 0.4844],
         [ 0.4844],
         ...,
         [ 3.8594],
         [ 3.8594],
         [ 3.8594]],

        [[-0.1138],
         [-0.1138],
         [-0.1138],
         ...,
         [ 2.7656],
         [ 2.7656],
         [ 2.7656]]], device='cuda:1', dtype=torch.bfloat16)
torch.Size([16, 560, 1]) reward_logits
tensor([ 1.3516,  0.8867,  1.7109, -0.3125,  1.5156,  1.7578,  0.8477,  4.6875,
         3.5156,  1.2188,  3.6875,  2.2188,  1.0625,  1.3906,  4.0312,  2.8438],
       device='cuda:1', dtype=torch.bfloat16)


 sequence_lengths,  tensor([537, 533, 530, 541, 542, 547, 539, 535, 550, 539, 550, 535, 540, 542,
        538, 548], device='cuda:1') torch.Size([16, 560, 1]) 507 line 971 


torch.Size([16])
tensor([[[ 2.0156],
         [ 2.0156],
         [ 2.0156],
         ...,
         [ 2.4844],
         [ 2.4844],
         [ 2.4844]],

        [[ 2.3750],
         [ 2.3750],
         [ 2.3750],
         ...,
         [ 4.8438],
         [ 4.8438],
         [ 4.8438]],

        [[ 1.1484],
         [ 1.1484],
         [ 1.1484],
         ...,
         [ 3.5938],
         [ 3.5938],
         [ 3.5938]],

        ...,

        [[-1.7578],
         [-1.7578],
         [-1.7578],
         ...,
         [-0.8125],
         [-0.8125],
         [-0.8125]],

        [[ 0.8984],
         [ 0.8984],
         [ 0.8984],
         ...,
         [ 1.3047],
         [ 1.3047],
         [ 1.3047]],

        [[ 0.6055],
         [ 0.6055],
         [ 0.6055],
         ...,
         [ 4.0938],
         [ 4.0938],
         [ 4.0938]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 553, 1]) reward_logits
tensor([ 2.6094,  4.7188,  3.3750, -1.2344,  5.3438,  5.0938,  0.8203,  1.5469,
         1.6094,  2.7031,  3.0469,  2.7656,  4.9062, -0.7188,  1.1172,  3.7188],
       device='cuda:0', dtype=torch.bfloat16)


 sequence_lengths,  tensor([542, 542, 538, 540, 529, 543, 531, 526, 522, 523, 541, 531, 537, 526,
        530, 539], device='cuda:0') torch.Size([16, 553, 1]) 500 line 971 


torch.Size([16])
tensor([[[ 0.4180],
         [ 0.4180],
         [ 0.4180],
         ...,
         [ 0.7812],
         [ 0.7812],
         [ 0.7812]],

        [[ 2.2656],
         [ 2.2656],
         [ 2.2656],
         ...,
         [ 6.0312],
         [ 6.0312],
         [ 6.0312]],

        [[-1.3203],
         [-1.3203],
         [-1.3203],
         ...,
         [ 0.4785],
         [ 0.4785],
         [ 0.4785]],

        ...,

        [[-0.8438],
         [-0.8438],
         [-0.8438],
         ...,
         [ 1.1250],
         [ 1.1250],
         [ 1.1250]],

        [[-0.1895],
         [-0.1895],
         [-0.1895],
         ...,
         [ 2.6875],
         [ 2.6875],
         [ 2.6875]],

        [[ 0.5859],
         [ 0.5859],
         [ 0.5859],
         ...,
         [ 1.7109],
         [ 1.7109],
         [ 1.7109]]], device='cuda:6', dtype=torch.bfloat16)
torch.Size([16, 541, 1]) reward_logits
tensor([ 0.8555,  5.7188,  0.5703,  4.1250, -2.0156,  1.8203, -0.5742,  2.5312,
         3.7656,  2.2188,  2.7344,  3.8438,  1.8047,  0.8516,  2.4844,  1.6250],
       device='cuda:6', dtype=torch.bfloat16)


 sequence_lengths,  tensor([515, 518, 517, 511, 540, 513, 518, 529, 514, 520, 524, 513, 517, 525,
        517, 514], device='cuda:6') torch.Size([16, 541, 1]) 488 line 971 


torch.Size([16])
tensor([[[ 0.8086],
         [ 0.8086],
         [ 0.8086],
         ...,
         [ 6.0938],
         [ 6.0938],
         [ 6.0938]],

        [[ 0.2578],
         [ 0.2578],
         [ 0.2578],
         ...,
         [ 0.4707],
         [ 0.4707],
         [ 0.4707]],

        [[ 3.9844],
         [ 3.9844],
         [ 3.9844],
         ...,
         [ 7.2188],
         [ 7.2188],
         [ 7.2188]],

        ...,

        [[-0.8125],
         [-0.8125],
         [-0.8125],
         ...,
         [ 1.2188],
         [ 1.2188],
         [ 1.2188]],

        [[ 1.0938],
         [ 1.0938],
         [ 1.0938],
         ...,
         [ 6.0000],
         [ 6.0000],
         [ 6.0000]],

        [[ 2.0469],
         [ 2.0469],
         [ 2.0469],
         ...,
         [ 5.5938],
         [ 5.5938],
         [ 5.5938]]], device='cuda:7', dtype=torch.bfloat16)
torch.Size([16, 565, 1]) reward_logits
tensor([ 5.8750,  0.4512,  7.0312, -2.0312,  2.9062,  2.2500,  3.8438,  0.4902,
         4.4688,  3.5469,  3.9531,  1.6328,  1.2500,  1.2734,  5.8125,  5.2500],
       device='cuda:7', dtype=torch.bfloat16)


 sequence_lengths,  tensor([545, 541, 534, 564, 539, 544, 562, 556, 540, 539, 535, 544, 541, 537,
        538, 539], device='cuda:7') torch.Size([16, 565, 1]) 512 line 971 


torch.Size([16])
tensor([[[ 0.2695],
         [ 0.2695],
         [ 0.2695],
         ...,
         [ 1.4688],
         [ 1.4688],
         [ 1.4688]],

        [[ 0.3262],
         [ 0.3262],
         [ 0.3262],
         ...,
         [ 0.9961],
         [ 0.9961],
         [ 0.9961]],

        [[ 1.1797],
         [ 1.1797],
         [ 1.1797],
         ...,
         [ 5.5625],
         [ 5.5625],
         [ 5.5625]],

        ...,

        [[ 0.1660],
         [ 0.1660],
         [ 0.1660],
         ...,
         [ 3.5625],
         [ 3.5625],
         [ 3.5625]],

        [[ 1.2344],
         [ 1.2344],
         [ 1.2344],
         ...,
         [ 2.4844],
         [ 2.4844],
         [ 2.4844]],

        [[-0.7031],
         [-0.7031],
         [-0.7031],
         ...,
         [ 2.4219],
         [ 2.4219],
         [ 2.4219]]], device='cuda:3', dtype=torch.bfloat16)
torch.Size([16, 561, 1]) reward_logits
tensor([ 1.5625,  0.6914,  5.0938,  0.8789,  3.0000,  3.1250,  1.9766,  3.8438,
         1.0469,  3.5156, -0.4375,  1.3828,  1.8359,  3.2969,  2.1719,  2.2031],
       device='cuda:3', dtype=torch.bfloat16)


 sequence_lengths,  tensor([526, 540, 543, 541, 552, 535, 541, 557, 535, 532, 538, 536, 533, 537,
        557, 536], device='cuda:3') torch.Size([16, 561, 1]) 508 line 971 


torch.Size([16])
torch.Size([16, 563]) mb_query_responses shape line 430
tensor([[[ 0.4258],
         [ 0.4258],
         [ 0.4258],
         ...,
         [ 1.2656],
         [ 0.6289],
         [-0.3672]],

        [[-0.1172],
         [-0.1172],
         [-0.1172],
         ...,
         [ 2.5312],
         [ 1.8750],
         [ 0.0413]],

        [[ 1.2812],
         [ 1.2812],
         [ 1.2812],
         ...,
         [ 0.0067],
         [-1.7266],
         [-2.5781]],

        ...,

        [[ 0.2334],
         [ 0.2334],
         [ 0.2334],
         ...,
         [-4.9062],
         [-4.4688],
         [-2.5625]],

        [[ 0.4453],
         [ 0.4453],
         [ 0.4453],
         ...,
         [ 0.2041],
         [-2.1406],
         [-1.5703]],

        [[ 1.0078],
         [ 1.0078],
         [ 1.0078],
         ...,
         [-2.0469],
         [-0.2119],
         [ 1.5547]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([-0.3672,  0.0413, -2.5781, -1.0000, -2.9062,  0.0981,  2.2656, -0.6055,
        -2.6875, -2.5000, -3.3750, -1.6172, -3.1875, -2.5625, -1.5703,  1.5547],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558, 558,
        558, 558], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:4') input_ids
torch.Size([16, 563, 2048]) hidden_states shape line 75
torch.Size([16, 563, 1]) logits shape line 77
tensor([[[-0.0222],
         [-0.0222],
         [-0.0222],
         ...,
         [ 0.5508],
         [ 0.5508],
         [ 0.5508]],

        [[ 0.4844],
         [ 0.4844],
         [ 0.4844],
         ...,
         [ 1.4922],
         [ 1.4922],
         [ 1.4922]],

        [[ 0.6016],
         [ 0.6016],
         [ 0.6016],
         ...,
         [ 1.1797],
         [ 1.1797],
         [ 1.1797]],

        ...,

        [[-0.0698],
         [-0.0698],
         [-0.0698],
         ...,
         [ 0.1953],
         [ 0.1953],
         [ 0.1953]],

        [[ 1.0859],
         [ 1.0859],
         [ 1.0859],
         ...,
         [ 4.7812],
         [ 4.7812],
         [ 4.7812]],

        [[ 0.3652],
         [ 0.3652],
         [ 0.3652],
         ...,
         [ 1.9531],
         [ 1.9531],
         [ 1.9531]]], device='cuda:4', dtype=torch.bfloat16)
torch.Size([16, 559, 1]) reward_logits
tensor([ 0.3770,  1.2109,  1.0625,  2.2656, -0.3477,  3.0469,  2.7500,  1.6094,
         0.1445,  2.4688,  2.4844,  3.1094,  2.0000, -0.1011,  4.7500,  1.8984],
       device='cuda:4', dtype=torch.bfloat16)


 sequence_lengths,  tensor([531, 535, 540, 551, 531, 543, 553, 541, 540, 535, 529, 544, 536, 536,
        540, 537], device='cuda:4') torch.Size([16, 559, 1]) 506 line 971 


torch.Size([16])
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 560]) mb_query_responses shape line 430
torch.Size([16, 553]) mb_query_responses shape line 430
torch.Size([16, 541]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 560, 1]) logits shape line 77
torch.Size([16, 541, 2048]) hidden_states shape line 75
torch.Size([16, 541, 1]) logits shape line 77
torch.Size([16, 553, 2048]) hidden_states shape line 75
torch.Size([16, 553, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 565]) mb_query_responses shape line 430
torch.Size([16, 565, 2048]) hidden_states shape line 75
torch.Size([16, 565, 1]) logits shape line 77
torch.Size([16, 559]) mb_query_responses shape line 430
torch.Size([16, 559, 2048]) hidden_states shape line 75
torch.Size([16, 559, 1]) logits shape line 77
torch.Size([16, 563]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 541]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 565]) mb_query_responses shape line 430
torch.Size([16, 559])torch.Size([16, 560])  mb_query_responses shape line 430mb_query_responses shape line 430

torch.Size([16, 553]) mb_query_responses shape line 430
torch.Size([16, 563, 2048]) hidden_states shape line 75
torch.Size([16, 563, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 541, 2048]) hidden_states shape line 75
torch.Size([16, 541, 1]) logits shape line 77
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 560, 1]) logits shape line 77
torch.Size([16, 565, 2048])torch.Size([16, 561, 2048]) torch.Size([16, 559, 2048]) hidden_states shape line 75 hidden_states shape line 75
hidden_states shape line 75

torch.Size([16, 565, 1]) logits shape line 77torch.Size([16, 561, 1])
torch.Size([16, 559, 1])  logits shape line 77logits shape line 77

torch.Size([16, 553, 2048]) hidden_states shape line 75
torch.Size([16, 553, 1]) logits shape line 77
torch.Size([16, 563]) mb_query_responses shape line 430torch.Size([16, 553])
torch.Size([16, 560])  mb_query_responses shape line 430mb_query_responses shape line 430

torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 541]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 565]) mb_query_responses shape line 430
torch.Size([16, 559]) mb_query_responses shape line 430
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 541, 2048]) torch.Size([16, 560, 1])torch.Size([16, 563, 2048])hidden_states shape line 75  
logits shape line 77hidden_states shape line 75

torch.Size([16, 541, 1])torch.Size([16, 563, 1])  logits shape line 77logits shape line 77

torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 553, 2048]) hidden_states shape line 75
torch.Size([16, 565, 2048]) hidden_states shape line 75
torch.Size([16, 553, 1]) logits shape line 77
torch.Size([16, 565, 1]) logits shape line 77
torch.Size([16, 559, 2048]) hidden_states shape line 75
torch.Size([16, 559, 1]) logits shape line 77
torch.Size([16, 560]) mb_query_responses shape line 430
torch.Size([16, 563])torch.Size([16, 553])  mb_query_responses shape line 430mb_query_responses shape line 430

torch.Size([16, 561])torch.Size([16, 541])  mb_query_responses shape line 430mb_query_responses shape line 430

torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 565]) mb_query_responses shape line 430
torch.Size([16, 559]) mb_query_responses shape line 430
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 560, 1]) logits shape line 77
torch.Size([16, 563, 2048]) hidden_states shape line 75
torch.Size([16, 563, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 559, 2048]) hidden_states shape line 75
torch.Size([16, 559, 1]) logits shape line 77
torch.Size([16, 541, 2048]) hidden_states shape line 75
torch.Size([16, 565, 2048])torch.Size([16, 541, 1])  hidden_states shape line 75logits shape line 77

torch.Size([16, 561, 2048])torch.Size([16, 565, 1])  hidden_states shape line 75logits shape line 77

torch.Size([16, 553, 2048])torch.Size([16, 561, 1])  hidden_states shape line 75logits shape line 77

torch.Size([16, 553, 1]) logits shape line 77
torch.Size([16, 560]) mb_query_responses shape line 430
torch.Size([16, 541]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 559]) mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 553]) mb_query_responses shape line 430
torch.Size([16, 563]) mb_query_responses shape line 430
torch.Size([16, 565]) mb_query_responses shape line 430
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 560, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 541, 2048]) hidden_states shape line 75
torch.Size([16, 541, 1]) logits shape line 77
torch.Size([16, 559, 2048]) hidden_states shape line 75
torch.Size([16, 559, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 553, 2048]) hidden_states shape line 75
torch.Size([16, 553, 1]) logits shape line 77
torch.Size([16, 565, 2048]) hidden_states shape line 75
torch.Size([16, 565, 1]) logits shape line 77
torch.Size([16, 563, 2048]) hidden_states shape line 75
torch.Size([16, 563, 1]) logits shape line 77
torch.Size([16, 563])torch.Size([16, 560])  torch.Size([16, 561])mb_query_responses shape line 430mb_query_responses shape line 430 torch.Size([16, 553])

mb_query_responses shape line 430 
mb_query_responses shape line 430
torch.Size([16, 561]) mb_query_responses shape line 430
torch.Size([16, 541]) torch.Size([16, 565])mb_query_responses shape line 430 
mb_query_responses shape line 430
torch.Size([16, 559]) mb_query_responses shape line 430
torch.Size([16, 560, 2048]) hidden_states shape line 75
torch.Size([16, 560, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 563, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 563, 1]) logits shape line 77
torch.Size([16, 561, 2048]) hidden_states shape line 75
torch.Size([16, 561, 1]) logits shape line 77
torch.Size([16, 553, 2048])torch.Size([16, 541, 2048])  hidden_states shape line 75hidden_states shape line 75

torch.Size([16, 541, 1])torch.Size([16, 553, 1])  logits shape line 77logits shape line 77

torch.Size([16, 565, 2048]) hidden_states shape line 75
torch.Size([16, 565, 1]) logits shape line 77
torch.Size([16, 559, 2048]) hidden_states shape line 75
torch.Size([16, 559, 1]) logits shape line 77
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/ubuntu/cooperate-LLM/trl/examples/scripts/ppo/ppo_tldr.py", line 117, in <module>
[rank4]:     trainer.train()
[rank4]:   File "/home/ubuntu/cooperate-LLM/trl/trl/trainer/ppov2_trainer.py", line 463, in train
[rank4]:     accelerator.backward(loss)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/accelerate/accelerator.py", line 2130, in backward
[rank4]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
[rank4]:     self.engine.backward(loss, **kwargs)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1976, in backward
[rank4]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2056, in backward
[rank4]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank4]:     scaled_loss.backward(retain_graph=retain_graph)
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.35 GiB. GPU  has a total capacity of 39.39 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 38.30 GiB memory in use. Of the allocated memory 31.57 GiB is allocated by PyTorch, and 4.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0710 04:03:47.348000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735262 closing signal SIGTERM
W0710 04:03:47.349000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735263 closing signal SIGTERM
W0710 04:03:47.349000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735264 closing signal SIGTERM
W0710 04:03:47.349000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735265 closing signal SIGTERM
W0710 04:03:47.349000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735267 closing signal SIGTERM
W0710 04:03:47.350000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735268 closing signal SIGTERM
W0710 04:03:47.350000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1735269 closing signal SIGTERM
E0710 04:03:48.620000 140567794980672 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 4 (pid: 1735266) of binary: /opt/conda/envs/cooperate/bin/python3.10
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/conda/envs/cooperate/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1082, in launch_command
    deepspeed_launcher(args)
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/accelerate/commands/launch.py", line 786, in deepspeed_launcher
    distrib_run.run(args)
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/cooperate/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
examples/scripts/ppo/ppo_tldr.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-10_04:03:47
  host      : ip-172-31-30-238.us-east-2.compute.internal
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 1735266)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
